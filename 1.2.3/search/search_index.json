{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"databooks is a package to ease the collaboration between data scientists using Jupyter notebooks , by reducing the number of git conflicts between different notebooks and resolution of git conflicts when encountered. The key features include: CLI tool Clear notebook metadata Resolve git conflicts Simple to use Simple API for using modelling and comparing notebooks using Pydantic Requirements databooks is built on top of: Python 3.7+ Typer Rich Pydantic GitPython Tomli Installation pip install databooks Usage Clear metadata Simply specify the paths for notebook files to remove metadata. By doing so, we can already avoid many of the conflicts. $ databooks meta [ OPTIONS ] PATHS... Fix git conflicts for notebooks Specify the paths for notebook files with conflicts to be fixed. Then, databooks finds the source notebooks that caused the conflicts and compares them (so no JSON manipulation!) $ databooks fix [ OPTIONS ] PATHS... Assert notebook metadata Specify paths of notebooks to be checked, an expression or recipe of what you'd like to enforce. databooks will run your checks and raise errors if any notebook does not comply with the desired metadata values. This advanced feature allows users to enforce cell tags, sequential cell execution, maximum number of cells, among many other things! Check out our docs for more! $ databooks assert [ OPTIONS ] PATHS... License This project is licensed under the terms of the MIT license.","title":"Overview"},{"location":"#requirements","text":"databooks is built on top of: Python 3.7+ Typer Rich Pydantic GitPython Tomli","title":"Requirements"},{"location":"#installation","text":"pip install databooks","title":"Installation"},{"location":"#usage","text":"","title":"Usage"},{"location":"#clear-metadata","text":"Simply specify the paths for notebook files to remove metadata. By doing so, we can already avoid many of the conflicts. $ databooks meta [ OPTIONS ] PATHS...","title":"Clear metadata"},{"location":"#fix-git-conflicts-for-notebooks","text":"Specify the paths for notebook files with conflicts to be fixed. Then, databooks finds the source notebooks that caused the conflicts and compares them (so no JSON manipulation!) $ databooks fix [ OPTIONS ] PATHS...","title":"Fix git conflicts for notebooks"},{"location":"#assert-notebook-metadata","text":"Specify paths of notebooks to be checked, an expression or recipe of what you'd like to enforce. databooks will run your checks and raise errors if any notebook does not comply with the desired metadata values. This advanced feature allows users to enforce cell tags, sequential cell execution, maximum number of cells, among many other things! Check out our docs for more! $ databooks assert [ OPTIONS ] PATHS...","title":"Assert notebook metadata"},{"location":"#license","text":"This project is licensed under the terms of the MIT license.","title":"License"},{"location":"API/","text":"Overview Though databooks was built to be used as a CLI application, you could also use the databooks data models to manipulate notebooks in other scripts. All the data models are based on Pydantic models . That means that we have a convenient way to read/write notebooks (JSON files) and ensure that the JSON is actually a valid notebook. By using databooks to parse your notebooks, you can also count on IDE support. Using databooks in python scripts is as simple as from databooks import JupyterNotebook notebook = JupyterNotebook . parse_file ( path = \"path/to/your/notebook\" ) assert list ( dict ( notebook ) . keys ()) == [ 'nbformat' , 'nbformat_minor' , 'metadata' , 'cells' ] For more information on the internal workings and what is implemented, see how it works .","title":"Overview"},{"location":"API/#overview","text":"Though databooks was built to be used as a CLI application, you could also use the databooks data models to manipulate notebooks in other scripts. All the data models are based on Pydantic models . That means that we have a convenient way to read/write notebooks (JSON files) and ensure that the JSON is actually a valid notebook. By using databooks to parse your notebooks, you can also count on IDE support. Using databooks in python scripts is as simple as from databooks import JupyterNotebook notebook = JupyterNotebook . parse_file ( path = \"path/to/your/notebook\" ) assert list ( dict ( notebook ) . keys ()) == [ 'nbformat' , 'nbformat_minor' , 'metadata' , 'cells' ] For more information on the internal workings and what is implemented, see how it works .","title":"Overview"},{"location":"CLI/","text":"databooks CLI tool to resolve git conflicts and remove metadata in notebooks. Usage : $ databooks [ OPTIONS ] COMMAND [ ARGS ] ... Options : --version --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. --help : Show this message and exit. Commands : assert : Assert notebook metadata has desired values. diff : Show differences between notebooks (not... fix : Fix git conflicts for notebooks. meta : Clear both notebook and cell metadata. show : Show rich representation of notebook. databooks assert Assert notebook metadata has desired values. Pass one (or multiple) strings or recipes. The available variables in scope include nb (notebook), raw_cells (notebook cells of raw type), md_cells (notebook cells of markdown type), code_cells (notebook cells of code type) and exec_cells (notebook cells of code type that were executed - have an execution count value). Recipes can be found on databooks.recipes.CookBook . Usage : $ databooks assert [ OPTIONS ] PATHS... Arguments : PATHS... : Path(s) of notebook files [required] Options : --ignore TEXT : Glob expression(s) of files to ignore [default: !*] -x, --expr TEXT : Expressions to assert on notebooks [default: ] -r, --recipe [has-tags|has-tags-code|max-cells|no-empty-code|seq-exec|seq-increase|startswith-md] : Common recipes of expressions - see https://databooks.dev/0.1.15/usage/overview/#recipes [default: ] -v, --verbose : Log processed files in console [default: False] -c, --config PATH : Get CLI options from configuration file --help : Show this message and exit databooks diff Show differences between notebooks (not implemented). Usage : $ databooks diff [ OPTIONS ] Options : --help : Show this message and exit. databooks fix Fix git conflicts for notebooks. Perform by getting the unmerged blobs from git index, comparing them and returning a valid notebook summarizing the differences - see git docs . Usage : $ databooks fix [ OPTIONS ] PATHS... Arguments : PATHS... : Path(s) of notebook files with conflicts [required] Options : --ignore TEXT : Glob expression(s) of files to ignore [default: !*] --metadata-head / --no-metadata-head : Whether or not to keep the metadata from the head/current notebook [default: True] --cells-head / --no-cells-head : Whether to keep the cells from the head/base notebook. Omit to keep both --cell-fields-ignore TEXT : Cell fields to remove before comparing cells [default: id, execution_count] -i, --interactive : Interactively resolve the conflicts (not implemented) [default: False] -v, --verbose : Log processed files in console [default: False] -c, --config PATH : Get CLI options from configuration file --help : Show this message and exit databooks meta Clear both notebook and cell metadata. Usage : $ databooks meta [ OPTIONS ] PATHS... Arguments : PATHS... : Path(s) of notebook files [required] Options : --ignore TEXT : Glob expression(s) of files to ignore [default: !*] --prefix TEXT : Prefix to add to filepath when writing files [default: ] --suffix TEXT : Suffix to add to filepath when writing files [default: ] --rm-outs / --no-rm-outs : Whether to remove cell outputs [default: False] --rm-exec / --no-rm-exec : Whether to remove the cell execution counts [default: True] --nb-meta-keep TEXT : Notebook metadata fields to keep [default: ] --cell-meta-keep TEXT : Cells metadata fields to keep [default: ] --cell-fields-keep TEXT : Other (excluding execution_counts and outputs ) cell fields to keep [default: ] -y, --yes : Confirm overwrite of files [default: False] --check : Don't write files but check whether there is unwanted metadata [default: False] -v, --verbose : Log processed files in console [default: False] -c, --config PATH : Get CLI options from configuration file --help : Show this message and exit databooks show Show rich representation of notebook. Usage : $ databooks show [ OPTIONS ] PATHS... Arguments : PATHS... : Path(s) of notebook files with conflicts [required] Options : --ignore TEXT : Glob expression(s) of files to ignore [default: !*] -p, --pager : Use pager instead of printing to terminal [default: False] -v, --verbose : Increase verbosity for debugging [default: False] -y, --yes : Show multiple files [default: False] -c, --config PATH : Get CLI options from configuration file --help : Show this message and exit","title":"CLI"},{"location":"CLI/#databooks","text":"CLI tool to resolve git conflicts and remove metadata in notebooks. Usage : $ databooks [ OPTIONS ] COMMAND [ ARGS ] ... Options : --version --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. --help : Show this message and exit. Commands : assert : Assert notebook metadata has desired values. diff : Show differences between notebooks (not... fix : Fix git conflicts for notebooks. meta : Clear both notebook and cell metadata. show : Show rich representation of notebook.","title":"databooks"},{"location":"CLI/#databooks-assert","text":"Assert notebook metadata has desired values. Pass one (or multiple) strings or recipes. The available variables in scope include nb (notebook), raw_cells (notebook cells of raw type), md_cells (notebook cells of markdown type), code_cells (notebook cells of code type) and exec_cells (notebook cells of code type that were executed - have an execution count value). Recipes can be found on databooks.recipes.CookBook . Usage : $ databooks assert [ OPTIONS ] PATHS... Arguments : PATHS... : Path(s) of notebook files [required] Options : --ignore TEXT : Glob expression(s) of files to ignore [default: !*] -x, --expr TEXT : Expressions to assert on notebooks [default: ] -r, --recipe [has-tags|has-tags-code|max-cells|no-empty-code|seq-exec|seq-increase|startswith-md] : Common recipes of expressions - see https://databooks.dev/0.1.15/usage/overview/#recipes [default: ] -v, --verbose : Log processed files in console [default: False] -c, --config PATH : Get CLI options from configuration file --help : Show this message and exit","title":"databooks assert"},{"location":"CLI/#databooks-diff","text":"Show differences between notebooks (not implemented). Usage : $ databooks diff [ OPTIONS ] Options : --help : Show this message and exit.","title":"databooks diff"},{"location":"CLI/#databooks-fix","text":"Fix git conflicts for notebooks. Perform by getting the unmerged blobs from git index, comparing them and returning a valid notebook summarizing the differences - see git docs . Usage : $ databooks fix [ OPTIONS ] PATHS... Arguments : PATHS... : Path(s) of notebook files with conflicts [required] Options : --ignore TEXT : Glob expression(s) of files to ignore [default: !*] --metadata-head / --no-metadata-head : Whether or not to keep the metadata from the head/current notebook [default: True] --cells-head / --no-cells-head : Whether to keep the cells from the head/base notebook. Omit to keep both --cell-fields-ignore TEXT : Cell fields to remove before comparing cells [default: id, execution_count] -i, --interactive : Interactively resolve the conflicts (not implemented) [default: False] -v, --verbose : Log processed files in console [default: False] -c, --config PATH : Get CLI options from configuration file --help : Show this message and exit","title":"databooks fix"},{"location":"CLI/#databooks-meta","text":"Clear both notebook and cell metadata. Usage : $ databooks meta [ OPTIONS ] PATHS... Arguments : PATHS... : Path(s) of notebook files [required] Options : --ignore TEXT : Glob expression(s) of files to ignore [default: !*] --prefix TEXT : Prefix to add to filepath when writing files [default: ] --suffix TEXT : Suffix to add to filepath when writing files [default: ] --rm-outs / --no-rm-outs : Whether to remove cell outputs [default: False] --rm-exec / --no-rm-exec : Whether to remove the cell execution counts [default: True] --nb-meta-keep TEXT : Notebook metadata fields to keep [default: ] --cell-meta-keep TEXT : Cells metadata fields to keep [default: ] --cell-fields-keep TEXT : Other (excluding execution_counts and outputs ) cell fields to keep [default: ] -y, --yes : Confirm overwrite of files [default: False] --check : Don't write files but check whether there is unwanted metadata [default: False] -v, --verbose : Log processed files in console [default: False] -c, --config PATH : Get CLI options from configuration file --help : Show this message and exit","title":"databooks meta"},{"location":"CLI/#databooks-show","text":"Show rich representation of notebook. Usage : $ databooks show [ OPTIONS ] PATHS... Arguments : PATHS... : Path(s) of notebook files with conflicts [required] Options : --ignore TEXT : Glob expression(s) of files to ignore [default: !*] -p, --pager : Use pager instead of printing to terminal [default: False] -v, --verbose : Increase verbosity for debugging [default: False] -y, --yes : Show multiple files [default: False] -c, --config PATH : Get CLI options from configuration file --help : Show this message and exit","title":"databooks show"},{"location":"affirm/","text":"Functions to safely evaluate strings and inspect notebook. DatabooksParser ( NodeVisitor ) AST parser that disallows unsafe nodes/values. Source code in databooks/affirm.py class DatabooksParser ( ast . NodeVisitor ): \"\"\"AST parser that disallows unsafe nodes/values.\"\"\" def __init__ ( self , ** variables : Any ) -> None : \"\"\"Instantiate with variables and callables (built-ins) scope.\"\"\" # https://github.com/python/mypy/issues/3728 self . builtins = { b . __name__ : b for b in _ALLOWED_BUILTINS } # type: ignore self . names = deepcopy ( variables ) or {} self . scope = { ** self . names , \"__builtins__\" : self . builtins , } @staticmethod def _prioritize ( field : Tuple [ str , Any ]) -> bool : \"\"\"Prioritize `ast.comprehension` nodes when expanding the AST tree.\"\"\" _ , value = field if not isinstance ( value , list ): return True return not any ( isinstance ( f , ast . comprehension ) for f in value ) @staticmethod def _allowed_attr ( obj : Any , attr : str , is_dynamic : bool = False ) -> None : \"\"\" Check that attribute is a key of `databooks.data_models.base.DatabooksBase`. If `obj` is an iterable and was computed dynamically (that is, not originally in scope but computed from a comprehension), check attributes for all elements in the iterable. \"\"\" allowed_attrs = list ( dict ( obj ) . keys ()) if isinstance ( obj , DatabooksBase ) else () if isinstance ( obj , abc . Iterable ) and is_dynamic : for el in obj : DatabooksParser . _allowed_attr ( obj = el , attr = attr ) else : if attr not in allowed_attrs : raise ValueError ( \"Expected attribute to be one of\" f \" ` { allowed_attrs } `, got ` { attr } ` for { obj } .\" ) def _get_iter ( self , node : ast . AST ) -> Iterable : \"\"\"Use `DatabooksParser.safe_eval_ast` to get the iterable object.\"\"\" tree = ast . Expression ( body = node ) return iter ( self . safe_eval_ast ( tree )) def generic_visit ( self , node : ast . AST ) -> None : \"\"\" Prioritize `ast.comprehension` nodes when expanding tree. Similar to `NodeVisitor.generic_visit`, but favor comprehensions when multiple nodes on the same level. In comprehensions, we have a generator argument that includes names that are stored. By visiting them first we avoid 'running into' unknown names. \"\"\" if not isinstance ( node , _ALLOWED_NODES ): raise ValueError ( f \"Invalid node ` { node } `.\" ) for field , value in sorted ( ast . iter_fields ( node ), key = self . _prioritize ): if isinstance ( value , list ): for item in value : if isinstance ( item , ast . AST ): self . visit ( item ) elif isinstance ( value , ast . AST ): self . visit ( value ) def visit_comprehension ( self , node : ast . comprehension ) -> None : \"\"\"Add variable from a comprehension to list of allowed names.\"\"\" if not isinstance ( node . target , ast . Name ): raise RuntimeError ( \"Expected `ast.comprehension`'s target to be `ast.Name`, got\" f \" `ast. { type ( node . target ) . __name__ } `.\" ) self . names [ node . target . id ] = self . _get_iter ( node . iter ) self . generic_visit ( node ) def visit_Attribute ( self , node : ast . Attribute ) -> None : \"\"\"Allow attributes for Pydantic fields only.\"\"\" if not isinstance ( node . value , ( ast . Attribute , ast . Name , ast . Subscript )): raise ValueError ( \"Expected attribute to be one of `ast.Name`, `ast.Attribute` or\" f \" `ast.Subscript`, got `ast. { type ( node . value ) . __name__ } `.\" ) if isinstance ( node . value , ast . Name ): self . _allowed_attr ( obj = self . names [ node . value . id ], attr = node . attr , is_dynamic = node . value . id in ( self . names . keys () - self . scope . keys ()), ) self . generic_visit ( node ) def visit_Name ( self , node : ast . Name ) -> None : \"\"\"Only allow names from scope or comprehension variables.\"\"\" valid_names = { ** self . names , ** self . builtins } if node . id not in valid_names : raise ValueError ( f \"Expected `name` to be one of ` { valid_names . keys () } `, got ` { node . id } `.\" ) self . generic_visit ( node ) def safe_eval_ast ( self , ast_tree : ast . AST ) -> Any : \"\"\"Evaluate safe AST trees only (raise errors otherwise).\"\"\" self . visit ( ast_tree ) exe = compile ( ast_tree , filename = \"\" , mode = \"eval\" ) return eval ( exe , self . scope ) def safe_eval ( self , src : str ) -> Any : \"\"\" Evaluate strings that are safe only (raise errors otherwise). A \"safe\" string or node provided may only consist of nodes in `databooks.affirm._ALLOWED_NODES` and built-ins from `databooks.affirm._ALLOWED_BUILTINS`. \"\"\" ast_tree = ast . parse ( src , mode = \"eval\" ) return self . safe_eval_ast ( ast_tree ) __init__ ( self , ** variables ) special Instantiate with variables and callables (built-ins) scope. Source code in databooks/affirm.py def __init__ ( self , ** variables : Any ) -> None : \"\"\"Instantiate with variables and callables (built-ins) scope.\"\"\" # https://github.com/python/mypy/issues/3728 self . builtins = { b . __name__ : b for b in _ALLOWED_BUILTINS } # type: ignore self . names = deepcopy ( variables ) or {} self . scope = { ** self . names , \"__builtins__\" : self . builtins , } generic_visit ( self , node ) Prioritize ast.comprehension nodes when expanding tree. Similar to NodeVisitor.generic_visit , but favor comprehensions when multiple nodes on the same level. In comprehensions, we have a generator argument that includes names that are stored. By visiting them first we avoid 'running into' unknown names. Source code in databooks/affirm.py def generic_visit ( self , node : ast . AST ) -> None : \"\"\" Prioritize `ast.comprehension` nodes when expanding tree. Similar to `NodeVisitor.generic_visit`, but favor comprehensions when multiple nodes on the same level. In comprehensions, we have a generator argument that includes names that are stored. By visiting them first we avoid 'running into' unknown names. \"\"\" if not isinstance ( node , _ALLOWED_NODES ): raise ValueError ( f \"Invalid node ` { node } `.\" ) for field , value in sorted ( ast . iter_fields ( node ), key = self . _prioritize ): if isinstance ( value , list ): for item in value : if isinstance ( item , ast . AST ): self . visit ( item ) elif isinstance ( value , ast . AST ): self . visit ( value ) safe_eval ( self , src ) Evaluate strings that are safe only (raise errors otherwise). A \"safe\" string or node provided may only consist of nodes in databooks.affirm._ALLOWED_NODES and built-ins from databooks.affirm._ALLOWED_BUILTINS . Source code in databooks/affirm.py def safe_eval ( self , src : str ) -> Any : \"\"\" Evaluate strings that are safe only (raise errors otherwise). A \"safe\" string or node provided may only consist of nodes in `databooks.affirm._ALLOWED_NODES` and built-ins from `databooks.affirm._ALLOWED_BUILTINS`. \"\"\" ast_tree = ast . parse ( src , mode = \"eval\" ) return self . safe_eval_ast ( ast_tree ) safe_eval_ast ( self , ast_tree ) Evaluate safe AST trees only (raise errors otherwise). Source code in databooks/affirm.py def safe_eval_ast ( self , ast_tree : ast . AST ) -> Any : \"\"\"Evaluate safe AST trees only (raise errors otherwise).\"\"\" self . visit ( ast_tree ) exe = compile ( ast_tree , filename = \"\" , mode = \"eval\" ) return eval ( exe , self . scope ) visit_Attribute ( self , node ) Allow attributes for Pydantic fields only. Source code in databooks/affirm.py def visit_Attribute ( self , node : ast . Attribute ) -> None : \"\"\"Allow attributes for Pydantic fields only.\"\"\" if not isinstance ( node . value , ( ast . Attribute , ast . Name , ast . Subscript )): raise ValueError ( \"Expected attribute to be one of `ast.Name`, `ast.Attribute` or\" f \" `ast.Subscript`, got `ast. { type ( node . value ) . __name__ } `.\" ) if isinstance ( node . value , ast . Name ): self . _allowed_attr ( obj = self . names [ node . value . id ], attr = node . attr , is_dynamic = node . value . id in ( self . names . keys () - self . scope . keys ()), ) self . generic_visit ( node ) visit_Name ( self , node ) Only allow names from scope or comprehension variables. Source code in databooks/affirm.py def visit_Name ( self , node : ast . Name ) -> None : \"\"\"Only allow names from scope or comprehension variables.\"\"\" valid_names = { ** self . names , ** self . builtins } if node . id not in valid_names : raise ValueError ( f \"Expected `name` to be one of ` { valid_names . keys () } `, got ` { node . id } `.\" ) self . generic_visit ( node ) visit_comprehension ( self , node ) Add variable from a comprehension to list of allowed names. Source code in databooks/affirm.py def visit_comprehension ( self , node : ast . comprehension ) -> None : \"\"\"Add variable from a comprehension to list of allowed names.\"\"\" if not isinstance ( node . target , ast . Name ): raise RuntimeError ( \"Expected `ast.comprehension`'s target to be `ast.Name`, got\" f \" `ast. { type ( node . target ) . __name__ } `.\" ) self . names [ node . target . id ] = self . _get_iter ( node . iter ) self . generic_visit ( node ) affirm ( nb_path , exprs , verbose = False ) Return whether notebook passed all checks (expressions). Parameters: Name Type Description Default nb_path Path Path of notebook file required exprs List[str] Expression with check to be evaluated on notebook required verbose bool Log failed tests for notebook False Returns: Type Description bool Evaluated expression cast as a bool Source code in databooks/affirm.py def affirm ( nb_path : Path , exprs : List [ str ], verbose : bool = False ) -> bool : \"\"\" Return whether notebook passed all checks (expressions). :param nb_path: Path of notebook file :param exprs: Expression with check to be evaluated on notebook :param verbose: Log failed tests for notebook :return: Evaluated expression cast as a `bool` \"\"\" if verbose : set_verbose ( logger ) nb = JupyterNotebook . parse_file ( nb_path ) variables : Dict [ str , Any ] = { \"nb\" : nb , \"raw_cells\" : [ c for c in nb . cells if c . cell_type == \"raw\" ], \"md_cells\" : [ c for c in nb . cells if c . cell_type == \"markdown\" ], \"code_cells\" : [ c for c in nb . cells if c . cell_type == \"code\" ], \"exec_cells\" : [ c for c in nb . cells if c . cell_type == \"code\" and c . execution_count is not None ], } databooks_parser = DatabooksParser ( ** variables ) is_ok = [ bool ( databooks_parser . safe_eval ( expr )) for expr in exprs ] n_fail = sum ([ not ok for ok in is_ok ]) logger . info ( f \" { nb_path } failed { n_fail } of { len ( is_ok ) } checks.\" ) logger . debug ( str ( nb_path ) + ( f \" failed { list ( compress ( exprs , ( not ok for ok in is_ok ))) } .\" if n_fail > 0 else \" succeeded all checks.\" ) ) return all ( is_ok ) affirm_all ( nb_paths , * , progress_callback =< function < lambda > at 0x7f6804449700 > , ** affirm_kwargs ) Clear metadata for multiple notebooks at notebooks and cell level. Parameters: Name Type Description Default nb_paths List[pathlib.Path] Paths of notebooks to assert metadata required progress_callback Callable[[], NoneType] Callback function to report progress <function <lambda> at 0x7f6804449700> affirm_kwargs Any Keyword arguments to be passed to databooks.affirm.affirm {} Returns: Type Description List[bool] Whether the notebooks contained or not the desired metadata Source code in databooks/affirm.py def affirm_all ( nb_paths : List [ Path ], * , progress_callback : Callable [[], None ] = lambda : None , ** affirm_kwargs : Any , ) -> List [ bool ]: \"\"\" Clear metadata for multiple notebooks at notebooks and cell level. :param nb_paths: Paths of notebooks to assert metadata :param progress_callback: Callback function to report progress :param affirm_kwargs: Keyword arguments to be passed to `databooks.affirm.affirm` :return: Whether the notebooks contained or not the desired metadata \"\"\" checks = [] for nb_path in nb_paths : checks . append ( affirm ( nb_path , ** affirm_kwargs )) progress_callback () return checks","title":"Affirm"},{"location":"affirm/#databooks.affirm.DatabooksParser","text":"AST parser that disallows unsafe nodes/values. Source code in databooks/affirm.py class DatabooksParser ( ast . NodeVisitor ): \"\"\"AST parser that disallows unsafe nodes/values.\"\"\" def __init__ ( self , ** variables : Any ) -> None : \"\"\"Instantiate with variables and callables (built-ins) scope.\"\"\" # https://github.com/python/mypy/issues/3728 self . builtins = { b . __name__ : b for b in _ALLOWED_BUILTINS } # type: ignore self . names = deepcopy ( variables ) or {} self . scope = { ** self . names , \"__builtins__\" : self . builtins , } @staticmethod def _prioritize ( field : Tuple [ str , Any ]) -> bool : \"\"\"Prioritize `ast.comprehension` nodes when expanding the AST tree.\"\"\" _ , value = field if not isinstance ( value , list ): return True return not any ( isinstance ( f , ast . comprehension ) for f in value ) @staticmethod def _allowed_attr ( obj : Any , attr : str , is_dynamic : bool = False ) -> None : \"\"\" Check that attribute is a key of `databooks.data_models.base.DatabooksBase`. If `obj` is an iterable and was computed dynamically (that is, not originally in scope but computed from a comprehension), check attributes for all elements in the iterable. \"\"\" allowed_attrs = list ( dict ( obj ) . keys ()) if isinstance ( obj , DatabooksBase ) else () if isinstance ( obj , abc . Iterable ) and is_dynamic : for el in obj : DatabooksParser . _allowed_attr ( obj = el , attr = attr ) else : if attr not in allowed_attrs : raise ValueError ( \"Expected attribute to be one of\" f \" ` { allowed_attrs } `, got ` { attr } ` for { obj } .\" ) def _get_iter ( self , node : ast . AST ) -> Iterable : \"\"\"Use `DatabooksParser.safe_eval_ast` to get the iterable object.\"\"\" tree = ast . Expression ( body = node ) return iter ( self . safe_eval_ast ( tree )) def generic_visit ( self , node : ast . AST ) -> None : \"\"\" Prioritize `ast.comprehension` nodes when expanding tree. Similar to `NodeVisitor.generic_visit`, but favor comprehensions when multiple nodes on the same level. In comprehensions, we have a generator argument that includes names that are stored. By visiting them first we avoid 'running into' unknown names. \"\"\" if not isinstance ( node , _ALLOWED_NODES ): raise ValueError ( f \"Invalid node ` { node } `.\" ) for field , value in sorted ( ast . iter_fields ( node ), key = self . _prioritize ): if isinstance ( value , list ): for item in value : if isinstance ( item , ast . AST ): self . visit ( item ) elif isinstance ( value , ast . AST ): self . visit ( value ) def visit_comprehension ( self , node : ast . comprehension ) -> None : \"\"\"Add variable from a comprehension to list of allowed names.\"\"\" if not isinstance ( node . target , ast . Name ): raise RuntimeError ( \"Expected `ast.comprehension`'s target to be `ast.Name`, got\" f \" `ast. { type ( node . target ) . __name__ } `.\" ) self . names [ node . target . id ] = self . _get_iter ( node . iter ) self . generic_visit ( node ) def visit_Attribute ( self , node : ast . Attribute ) -> None : \"\"\"Allow attributes for Pydantic fields only.\"\"\" if not isinstance ( node . value , ( ast . Attribute , ast . Name , ast . Subscript )): raise ValueError ( \"Expected attribute to be one of `ast.Name`, `ast.Attribute` or\" f \" `ast.Subscript`, got `ast. { type ( node . value ) . __name__ } `.\" ) if isinstance ( node . value , ast . Name ): self . _allowed_attr ( obj = self . names [ node . value . id ], attr = node . attr , is_dynamic = node . value . id in ( self . names . keys () - self . scope . keys ()), ) self . generic_visit ( node ) def visit_Name ( self , node : ast . Name ) -> None : \"\"\"Only allow names from scope or comprehension variables.\"\"\" valid_names = { ** self . names , ** self . builtins } if node . id not in valid_names : raise ValueError ( f \"Expected `name` to be one of ` { valid_names . keys () } `, got ` { node . id } `.\" ) self . generic_visit ( node ) def safe_eval_ast ( self , ast_tree : ast . AST ) -> Any : \"\"\"Evaluate safe AST trees only (raise errors otherwise).\"\"\" self . visit ( ast_tree ) exe = compile ( ast_tree , filename = \"\" , mode = \"eval\" ) return eval ( exe , self . scope ) def safe_eval ( self , src : str ) -> Any : \"\"\" Evaluate strings that are safe only (raise errors otherwise). A \"safe\" string or node provided may only consist of nodes in `databooks.affirm._ALLOWED_NODES` and built-ins from `databooks.affirm._ALLOWED_BUILTINS`. \"\"\" ast_tree = ast . parse ( src , mode = \"eval\" ) return self . safe_eval_ast ( ast_tree )","title":"DatabooksParser"},{"location":"affirm/#databooks.affirm.DatabooksParser.__init__","text":"Instantiate with variables and callables (built-ins) scope. Source code in databooks/affirm.py def __init__ ( self , ** variables : Any ) -> None : \"\"\"Instantiate with variables and callables (built-ins) scope.\"\"\" # https://github.com/python/mypy/issues/3728 self . builtins = { b . __name__ : b for b in _ALLOWED_BUILTINS } # type: ignore self . names = deepcopy ( variables ) or {} self . scope = { ** self . names , \"__builtins__\" : self . builtins , }","title":"__init__()"},{"location":"affirm/#databooks.affirm.DatabooksParser.generic_visit","text":"Prioritize ast.comprehension nodes when expanding tree. Similar to NodeVisitor.generic_visit , but favor comprehensions when multiple nodes on the same level. In comprehensions, we have a generator argument that includes names that are stored. By visiting them first we avoid 'running into' unknown names. Source code in databooks/affirm.py def generic_visit ( self , node : ast . AST ) -> None : \"\"\" Prioritize `ast.comprehension` nodes when expanding tree. Similar to `NodeVisitor.generic_visit`, but favor comprehensions when multiple nodes on the same level. In comprehensions, we have a generator argument that includes names that are stored. By visiting them first we avoid 'running into' unknown names. \"\"\" if not isinstance ( node , _ALLOWED_NODES ): raise ValueError ( f \"Invalid node ` { node } `.\" ) for field , value in sorted ( ast . iter_fields ( node ), key = self . _prioritize ): if isinstance ( value , list ): for item in value : if isinstance ( item , ast . AST ): self . visit ( item ) elif isinstance ( value , ast . AST ): self . visit ( value )","title":"generic_visit()"},{"location":"affirm/#databooks.affirm.DatabooksParser.safe_eval","text":"Evaluate strings that are safe only (raise errors otherwise). A \"safe\" string or node provided may only consist of nodes in databooks.affirm._ALLOWED_NODES and built-ins from databooks.affirm._ALLOWED_BUILTINS . Source code in databooks/affirm.py def safe_eval ( self , src : str ) -> Any : \"\"\" Evaluate strings that are safe only (raise errors otherwise). A \"safe\" string or node provided may only consist of nodes in `databooks.affirm._ALLOWED_NODES` and built-ins from `databooks.affirm._ALLOWED_BUILTINS`. \"\"\" ast_tree = ast . parse ( src , mode = \"eval\" ) return self . safe_eval_ast ( ast_tree )","title":"safe_eval()"},{"location":"affirm/#databooks.affirm.DatabooksParser.safe_eval_ast","text":"Evaluate safe AST trees only (raise errors otherwise). Source code in databooks/affirm.py def safe_eval_ast ( self , ast_tree : ast . AST ) -> Any : \"\"\"Evaluate safe AST trees only (raise errors otherwise).\"\"\" self . visit ( ast_tree ) exe = compile ( ast_tree , filename = \"\" , mode = \"eval\" ) return eval ( exe , self . scope )","title":"safe_eval_ast()"},{"location":"affirm/#databooks.affirm.DatabooksParser.visit_Attribute","text":"Allow attributes for Pydantic fields only. Source code in databooks/affirm.py def visit_Attribute ( self , node : ast . Attribute ) -> None : \"\"\"Allow attributes for Pydantic fields only.\"\"\" if not isinstance ( node . value , ( ast . Attribute , ast . Name , ast . Subscript )): raise ValueError ( \"Expected attribute to be one of `ast.Name`, `ast.Attribute` or\" f \" `ast.Subscript`, got `ast. { type ( node . value ) . __name__ } `.\" ) if isinstance ( node . value , ast . Name ): self . _allowed_attr ( obj = self . names [ node . value . id ], attr = node . attr , is_dynamic = node . value . id in ( self . names . keys () - self . scope . keys ()), ) self . generic_visit ( node )","title":"visit_Attribute()"},{"location":"affirm/#databooks.affirm.DatabooksParser.visit_Name","text":"Only allow names from scope or comprehension variables. Source code in databooks/affirm.py def visit_Name ( self , node : ast . Name ) -> None : \"\"\"Only allow names from scope or comprehension variables.\"\"\" valid_names = { ** self . names , ** self . builtins } if node . id not in valid_names : raise ValueError ( f \"Expected `name` to be one of ` { valid_names . keys () } `, got ` { node . id } `.\" ) self . generic_visit ( node )","title":"visit_Name()"},{"location":"affirm/#databooks.affirm.DatabooksParser.visit_comprehension","text":"Add variable from a comprehension to list of allowed names. Source code in databooks/affirm.py def visit_comprehension ( self , node : ast . comprehension ) -> None : \"\"\"Add variable from a comprehension to list of allowed names.\"\"\" if not isinstance ( node . target , ast . Name ): raise RuntimeError ( \"Expected `ast.comprehension`'s target to be `ast.Name`, got\" f \" `ast. { type ( node . target ) . __name__ } `.\" ) self . names [ node . target . id ] = self . _get_iter ( node . iter ) self . generic_visit ( node )","title":"visit_comprehension()"},{"location":"affirm/#databooks.affirm.affirm","text":"Return whether notebook passed all checks (expressions). Parameters: Name Type Description Default nb_path Path Path of notebook file required exprs List[str] Expression with check to be evaluated on notebook required verbose bool Log failed tests for notebook False Returns: Type Description bool Evaluated expression cast as a bool Source code in databooks/affirm.py def affirm ( nb_path : Path , exprs : List [ str ], verbose : bool = False ) -> bool : \"\"\" Return whether notebook passed all checks (expressions). :param nb_path: Path of notebook file :param exprs: Expression with check to be evaluated on notebook :param verbose: Log failed tests for notebook :return: Evaluated expression cast as a `bool` \"\"\" if verbose : set_verbose ( logger ) nb = JupyterNotebook . parse_file ( nb_path ) variables : Dict [ str , Any ] = { \"nb\" : nb , \"raw_cells\" : [ c for c in nb . cells if c . cell_type == \"raw\" ], \"md_cells\" : [ c for c in nb . cells if c . cell_type == \"markdown\" ], \"code_cells\" : [ c for c in nb . cells if c . cell_type == \"code\" ], \"exec_cells\" : [ c for c in nb . cells if c . cell_type == \"code\" and c . execution_count is not None ], } databooks_parser = DatabooksParser ( ** variables ) is_ok = [ bool ( databooks_parser . safe_eval ( expr )) for expr in exprs ] n_fail = sum ([ not ok for ok in is_ok ]) logger . info ( f \" { nb_path } failed { n_fail } of { len ( is_ok ) } checks.\" ) logger . debug ( str ( nb_path ) + ( f \" failed { list ( compress ( exprs , ( not ok for ok in is_ok ))) } .\" if n_fail > 0 else \" succeeded all checks.\" ) ) return all ( is_ok )","title":"affirm()"},{"location":"affirm/#databooks.affirm.affirm_all","text":"Clear metadata for multiple notebooks at notebooks and cell level. Parameters: Name Type Description Default nb_paths List[pathlib.Path] Paths of notebooks to assert metadata required progress_callback Callable[[], NoneType] Callback function to report progress <function <lambda> at 0x7f6804449700> affirm_kwargs Any Keyword arguments to be passed to databooks.affirm.affirm {} Returns: Type Description List[bool] Whether the notebooks contained or not the desired metadata Source code in databooks/affirm.py def affirm_all ( nb_paths : List [ Path ], * , progress_callback : Callable [[], None ] = lambda : None , ** affirm_kwargs : Any , ) -> List [ bool ]: \"\"\" Clear metadata for multiple notebooks at notebooks and cell level. :param nb_paths: Paths of notebooks to assert metadata :param progress_callback: Callback function to report progress :param affirm_kwargs: Keyword arguments to be passed to `databooks.affirm.affirm` :return: Whether the notebooks contained or not the desired metadata \"\"\" checks = [] for nb_path in nb_paths : checks . append ( affirm ( nb_path , ** affirm_kwargs )) progress_callback () return checks","title":"affirm_all()"},{"location":"alternatives/","text":"Alternatives and comparisons There are many tools to improve the development experience with notebooks. databooks also takes inspiration from other existing packages. nb-clean nb-clean provides a CLI tool to clear notebook metadata from Jupyter notebooks. Differences: nb-clean does not offer a way to resolve conflicts. nbdime nbdime stands for \"notebook diff and merge\". It offers a way to compare and display the differences in the terminal. It offers a way to gracefully merge notebooks. Differences: nb-dime does not offer a way to remove metadata. databooks also fixes git merge conflicts instead of offering a way to do git merges (\"ask for forgiveness not permission\"). nbdev nbdev offers a way to use notebooks to develop python packages, converting notebooks to python scripts and documentation. Also offers installation of pre-commit hooks to remove notebook metadata and a way to resolve git conflicts. Differences: nbdev is closer to an opinionated template for developing packages with notebooks. databooks is a configurable CLI tool for metadata removal and conflict resolution.","title":"Alternatives and comparisons"},{"location":"alternatives/#alternatives-and-comparisons","text":"There are many tools to improve the development experience with notebooks. databooks also takes inspiration from other existing packages.","title":"Alternatives and comparisons"},{"location":"alternatives/#nb-clean","text":"nb-clean provides a CLI tool to clear notebook metadata from Jupyter notebooks. Differences: nb-clean does not offer a way to resolve conflicts.","title":"nb-clean"},{"location":"alternatives/#nbdime","text":"nbdime stands for \"notebook diff and merge\". It offers a way to compare and display the differences in the terminal. It offers a way to gracefully merge notebooks. Differences: nb-dime does not offer a way to remove metadata. databooks also fixes git merge conflicts instead of offering a way to do git merges (\"ask for forgiveness not permission\").","title":"nbdime"},{"location":"alternatives/#nbdev","text":"nbdev offers a way to use notebooks to develop python packages, converting notebooks to python scripts and documentation. Also offers installation of pre-commit hooks to remove notebook metadata and a way to resolve git conflicts. Differences: nbdev is closer to an opinionated template for developing packages with notebooks. databooks is a configurable CLI tool for metadata removal and conflict resolution.","title":"nbdev"},{"location":"common/","text":"Common set of miscellaneous functions. expand_paths ( paths , * , ignore = [ '!*' ], rglob = '*.ipynb' ) Get paths of existing file from list of directory or file paths. Parameters: Name Type Description Default paths List[pathlib.Path] Paths to consider (can be directories or files) required ignore List[str] Glob expressions of files to ignore ['!*'] rglob str Glob expression for expanding directory paths and filtering out existing file paths (i.e.: to retrieve only notebooks) '*.ipynb' Returns: Type Description List[pathlib.Path] List of existing file paths Source code in databooks/common.py def expand_paths ( paths : List [ Path ], * , ignore : List [ str ] = [ \"!*\" ], rglob : str = \"*.ipynb\" ) -> List [ Path ]: \"\"\" Get paths of existing file from list of directory or file paths. :param paths: Paths to consider (can be directories or files) :param ignore: Glob expressions of files to ignore :param rglob: Glob expression for expanding directory paths and filtering out existing file paths (i.e.: to retrieve only notebooks) :return: List of existing file paths \"\"\" filepaths = list ( chain . from_iterable ( list ( path . rglob ( rglob )) if path . is_dir () else [ path ] for path in paths ) ) valid_filepaths = [ p for p in filepaths if not any ( p . match ( i ) for i in ignore ) and p . is_file () and p . match ( rglob ) ] if not valid_filepaths : logger . debug ( f \"There are no files in { paths } (ignoring { ignore } ) that match ` { rglob } `.\" ) return valid_filepaths find_common_parent ( paths ) Find common parent amongst several file paths. Source code in databooks/common.py def find_common_parent ( paths : Iterable [ Path ]) -> Path : \"\"\"Find common parent amongst several file paths.\"\"\" if not paths : raise ValueError ( f \"Expected non-empty `paths`, got { paths } .\" ) return max ( set . intersection ( * [ set ( p . resolve () . parents ) for p in paths ])) find_obj ( obj_name , start , finish , is_dir = False ) Recursively find file along directory path, from the end (child) directory to start. Parameters: Name Type Description Default obj_name str File name to locate required start Path Start (parent) directory required finish Path Finish (child) directory required is_dir bool Whether object is a directory or a file False Returns: Type Description Optional[pathlib.Path] File path Source code in databooks/common.py def find_obj ( obj_name : str , start : Path , finish : Path , is_dir : bool = False ) -> Optional [ Path ]: \"\"\" Recursively find file along directory path, from the end (child) directory to start. :param obj_name: File name to locate :param start: Start (parent) directory :param finish: Finish (child) directory :param is_dir: Whether object is a directory or a file :return: File path \"\"\" if not start . is_dir () or not finish . is_dir (): raise ValueError ( \"Parameters `start` and `finish` must be directories.\" ) if start . resolve () not in [ finish , * finish . resolve () . parents ]: logger . debug ( f \"Parameter `start` is not a parent directory of `finish` (for { start } and\" f \" { finish } ). Cannot find { obj_name } .\" ) return None is_obj = ( finish / obj_name ) . is_dir () if is_dir else ( finish / obj_name ) . is_file () if is_obj : return finish / obj_name elif finish . samefile ( start ): logger . debug ( f \" { obj_name } not found between { start } and { finish } .\" ) return None else : return find_obj ( obj_name = obj_name , start = start , finish = finish . parent , is_dir = is_dir )","title":"Common utils"},{"location":"common/#databooks.common.expand_paths","text":"Get paths of existing file from list of directory or file paths. Parameters: Name Type Description Default paths List[pathlib.Path] Paths to consider (can be directories or files) required ignore List[str] Glob expressions of files to ignore ['!*'] rglob str Glob expression for expanding directory paths and filtering out existing file paths (i.e.: to retrieve only notebooks) '*.ipynb' Returns: Type Description List[pathlib.Path] List of existing file paths Source code in databooks/common.py def expand_paths ( paths : List [ Path ], * , ignore : List [ str ] = [ \"!*\" ], rglob : str = \"*.ipynb\" ) -> List [ Path ]: \"\"\" Get paths of existing file from list of directory or file paths. :param paths: Paths to consider (can be directories or files) :param ignore: Glob expressions of files to ignore :param rglob: Glob expression for expanding directory paths and filtering out existing file paths (i.e.: to retrieve only notebooks) :return: List of existing file paths \"\"\" filepaths = list ( chain . from_iterable ( list ( path . rglob ( rglob )) if path . is_dir () else [ path ] for path in paths ) ) valid_filepaths = [ p for p in filepaths if not any ( p . match ( i ) for i in ignore ) and p . is_file () and p . match ( rglob ) ] if not valid_filepaths : logger . debug ( f \"There are no files in { paths } (ignoring { ignore } ) that match ` { rglob } `.\" ) return valid_filepaths","title":"expand_paths()"},{"location":"common/#databooks.common.find_common_parent","text":"Find common parent amongst several file paths. Source code in databooks/common.py def find_common_parent ( paths : Iterable [ Path ]) -> Path : \"\"\"Find common parent amongst several file paths.\"\"\" if not paths : raise ValueError ( f \"Expected non-empty `paths`, got { paths } .\" ) return max ( set . intersection ( * [ set ( p . resolve () . parents ) for p in paths ]))","title":"find_common_parent()"},{"location":"common/#databooks.common.find_obj","text":"Recursively find file along directory path, from the end (child) directory to start. Parameters: Name Type Description Default obj_name str File name to locate required start Path Start (parent) directory required finish Path Finish (child) directory required is_dir bool Whether object is a directory or a file False Returns: Type Description Optional[pathlib.Path] File path Source code in databooks/common.py def find_obj ( obj_name : str , start : Path , finish : Path , is_dir : bool = False ) -> Optional [ Path ]: \"\"\" Recursively find file along directory path, from the end (child) directory to start. :param obj_name: File name to locate :param start: Start (parent) directory :param finish: Finish (child) directory :param is_dir: Whether object is a directory or a file :return: File path \"\"\" if not start . is_dir () or not finish . is_dir (): raise ValueError ( \"Parameters `start` and `finish` must be directories.\" ) if start . resolve () not in [ finish , * finish . resolve () . parents ]: logger . debug ( f \"Parameter `start` is not a parent directory of `finish` (for { start } and\" f \" { finish } ). Cannot find { obj_name } .\" ) return None is_obj = ( finish / obj_name ) . is_dir () if is_dir else ( finish / obj_name ) . is_file () if is_obj : return finish / obj_name elif finish . samefile ( start ): logger . debug ( f \" { obj_name } not found between { start } and { finish } .\" ) return None else : return find_obj ( obj_name = obj_name , start = start , finish = finish . parent , is_dir = is_dir )","title":"find_obj()"},{"location":"config/","text":"Configuration functions, and settings objects. get_config ( target_paths , config_filename ) Find configuration file from CLI target paths. Source code in databooks/config.py def get_config ( target_paths : List [ Path ], config_filename : str ) -> Optional [ Path ]: \"\"\"Find configuration file from CLI target paths.\"\"\" common_parent = find_common_parent ( paths = target_paths ) repo_dir = get_repo () . working_dir return find_obj ( obj_name = config_filename , start = Path ( repo_dir ) if repo_dir is not None else Path ( common_parent . anchor ), finish = common_parent , )","title":"Configuration"},{"location":"config/#databooks.config.get_config","text":"Find configuration file from CLI target paths. Source code in databooks/config.py def get_config ( target_paths : List [ Path ], config_filename : str ) -> Optional [ Path ]: \"\"\"Find configuration file from CLI target paths.\"\"\" common_parent = find_common_parent ( paths = target_paths ) repo_dir = get_repo () . working_dir return find_obj ( obj_name = config_filename , start = Path ( repo_dir ) if repo_dir is not None else Path ( common_parent . anchor ), finish = common_parent , )","title":"get_config()"},{"location":"conflicts/","text":"Functions to resolve any git conflicts between notebooks. conflict2nb ( conflict_file , * , meta_first = True , cells_first = None , cell_fields_ignore = ( 'id' , 'execution_count' ), ignore_none = True , verbose = False ) Merge diffs from conflicts and return valid a notebook. Parameters: Name Type Description Default conflict_file ConflictFile A databooks.git_utils.ConflictFile with conflicts required meta_first bool Whether to keep the metadata of the first or last notebook True cells_first Optional[bool] Whether to keep the cells of the first or last notebook None ignore_none bool Keep all metadata fields even if it's included in only one notebook True cell_fields_ignore Sequence[str] Fields to remove before comparing notebooks - i.e.: cell IDs or execution counts may not want to be considered ('id', 'execution_count') verbose bool Log written files and metadata conflicts False Returns: Type Description JupyterNotebook Resolved conflicts as a databooks.data_models.notebook.JupyterNotebook model Source code in databooks/conflicts.py def conflict2nb ( conflict_file : ConflictFile , * , meta_first : bool = True , cells_first : Optional [ bool ] = None , cell_fields_ignore : Sequence [ str ] = ( \"id\" , \"execution_count\" ), ignore_none : bool = True , verbose : bool = False , ) -> JupyterNotebook : \"\"\" Merge diffs from conflicts and return valid a notebook. :param conflict_file: A `databooks.git_utils.ConflictFile` with conflicts :param meta_first: Whether to keep the metadata of the first or last notebook :param cells_first: Whether to keep the cells of the first or last notebook :param ignore_none: Keep all metadata fields even if it's included in only one notebook :param cell_fields_ignore: Fields to remove before comparing notebooks - i.e.: cell IDs or execution counts may not want to be considered :param verbose: Log written files and metadata conflicts :return: Resolved conflicts as a `databooks.data_models.notebook.JupyterNotebook` model \"\"\" if verbose : set_verbose ( logger ) nb_1 = JupyterNotebook . parse_raw ( conflict_file . first_contents ) nb_2 = JupyterNotebook . parse_raw ( conflict_file . last_contents ) if nb_1 . metadata != nb_2 . metadata : msg = ( f \"Notebook metadata conflict for { conflict_file . filename } . Keeping \" + \"first.\" if meta_first else \"last.\" ) logger . debug ( msg ) if cell_fields_ignore : for cells in ( nb_1 . cells , nb_2 . cells ): for cell in cells : cell . clear_fields ( cell_metadata_remove = [], cell_remove_fields = cell_fields_ignore ) diff_nb = nb_1 - nb_2 nb = diff_nb . resolve ( ignore_none = ignore_none , keep_first = meta_first , keep_first_cells = cells_first , first_id = conflict_file . first_log , last_id = conflict_file . last_log , ) if not isinstance ( nb , JupyterNotebook ): raise RuntimeError ( f \"Expected `databooks.JupyterNotebook`, got { type ( nb ) } .\" ) logger . debug ( f \"Resolved conflicts in { conflict_file . filename } .\" ) return nb conflicts2nbs ( conflict_files , * , progress_callback =< function < lambda > at 0x7f6804bf7ee0 > , ** conflict2nb_kwargs ) Get notebooks from conflicts. Wrap databooks.conflicts.conflict2nb to write notebooks to list of databooks.git_utils.ConflictFile . Parameters: Name Type Description Default conflict_files List[ConflictFile] Files with source conflict files and one-liner git logs required progress_callback Callable[[], None] Callback function to report progress <function <lambda> at 0x7f6804bf7ee0> conflict2nb_kwargs Any Keyword arguments to be passed to databooks.conflicts.conflict2nb {} Returns: Type Description None Source code in databooks/conflicts.py def conflicts2nbs ( conflict_files : List [ ConflictFile ], * , progress_callback : Callable [[], None ] = lambda : None , ** conflict2nb_kwargs : Any , ) -> None : \"\"\" Get notebooks from conflicts. Wrap `databooks.conflicts.conflict2nb` to write notebooks to list of `databooks.git_utils.ConflictFile`. :param conflict_files: Files with source conflict files and one-liner git logs :param progress_callback: Callback function to report progress :param conflict2nb_kwargs: Keyword arguments to be passed to `databooks.conflicts.conflict2nb` :return: \"\"\" for conflict in conflict_files : nb = conflict2nb ( conflict , ** conflict2nb_kwargs ) nb . write ( path = conflict . filename , overwrite = True ) progress_callback () path2conflicts ( nb_paths , repo = None ) Get the difference model from the path based on the git conflict information. Parameters: Name Type Description Default nb_paths List[Path] Path to file with conflicts (must be notebook paths) required repo Optional[Repo] The git repo to look for conflicts None Returns: Type Description List[ConflictFile] Generator of DiffModel s, to be resolved Source code in databooks/conflicts.py def path2conflicts ( nb_paths : List [ Path ], repo : Optional [ Repo ] = None ) -> List [ ConflictFile ]: \"\"\" Get the difference model from the path based on the git conflict information. :param nb_paths: Path to file with conflicts (must be notebook paths) :param repo: The git repo to look for conflicts :return: Generator of `DiffModel`s, to be resolved \"\"\" if any ( nb_path . suffix not in ( \"\" , \".ipynb\" ) for nb_path in nb_paths ): raise ValueError ( \"Expected either notebook files, a directory or glob expression.\" ) common_parent = find_common_parent ( nb_paths ) repo = get_repo ( common_parent ) if repo is None else repo return [ file for file in get_conflict_blobs ( repo = repo ) if any ( file . filename . match ( str ( p . name )) for p in nb_paths ) ]","title":"Conflicts"},{"location":"conflicts/#databooks.conflicts.conflict2nb","text":"Merge diffs from conflicts and return valid a notebook. Parameters: Name Type Description Default conflict_file ConflictFile A databooks.git_utils.ConflictFile with conflicts required meta_first bool Whether to keep the metadata of the first or last notebook True cells_first Optional[bool] Whether to keep the cells of the first or last notebook None ignore_none bool Keep all metadata fields even if it's included in only one notebook True cell_fields_ignore Sequence[str] Fields to remove before comparing notebooks - i.e.: cell IDs or execution counts may not want to be considered ('id', 'execution_count') verbose bool Log written files and metadata conflicts False Returns: Type Description JupyterNotebook Resolved conflicts as a databooks.data_models.notebook.JupyterNotebook model Source code in databooks/conflicts.py def conflict2nb ( conflict_file : ConflictFile , * , meta_first : bool = True , cells_first : Optional [ bool ] = None , cell_fields_ignore : Sequence [ str ] = ( \"id\" , \"execution_count\" ), ignore_none : bool = True , verbose : bool = False , ) -> JupyterNotebook : \"\"\" Merge diffs from conflicts and return valid a notebook. :param conflict_file: A `databooks.git_utils.ConflictFile` with conflicts :param meta_first: Whether to keep the metadata of the first or last notebook :param cells_first: Whether to keep the cells of the first or last notebook :param ignore_none: Keep all metadata fields even if it's included in only one notebook :param cell_fields_ignore: Fields to remove before comparing notebooks - i.e.: cell IDs or execution counts may not want to be considered :param verbose: Log written files and metadata conflicts :return: Resolved conflicts as a `databooks.data_models.notebook.JupyterNotebook` model \"\"\" if verbose : set_verbose ( logger ) nb_1 = JupyterNotebook . parse_raw ( conflict_file . first_contents ) nb_2 = JupyterNotebook . parse_raw ( conflict_file . last_contents ) if nb_1 . metadata != nb_2 . metadata : msg = ( f \"Notebook metadata conflict for { conflict_file . filename } . Keeping \" + \"first.\" if meta_first else \"last.\" ) logger . debug ( msg ) if cell_fields_ignore : for cells in ( nb_1 . cells , nb_2 . cells ): for cell in cells : cell . clear_fields ( cell_metadata_remove = [], cell_remove_fields = cell_fields_ignore ) diff_nb = nb_1 - nb_2 nb = diff_nb . resolve ( ignore_none = ignore_none , keep_first = meta_first , keep_first_cells = cells_first , first_id = conflict_file . first_log , last_id = conflict_file . last_log , ) if not isinstance ( nb , JupyterNotebook ): raise RuntimeError ( f \"Expected `databooks.JupyterNotebook`, got { type ( nb ) } .\" ) logger . debug ( f \"Resolved conflicts in { conflict_file . filename } .\" ) return nb","title":"conflict2nb()"},{"location":"conflicts/#databooks.conflicts.conflicts2nbs","text":"Get notebooks from conflicts. Wrap databooks.conflicts.conflict2nb to write notebooks to list of databooks.git_utils.ConflictFile . Parameters: Name Type Description Default conflict_files List[ConflictFile] Files with source conflict files and one-liner git logs required progress_callback Callable[[], None] Callback function to report progress <function <lambda> at 0x7f6804bf7ee0> conflict2nb_kwargs Any Keyword arguments to be passed to databooks.conflicts.conflict2nb {} Returns: Type Description None Source code in databooks/conflicts.py def conflicts2nbs ( conflict_files : List [ ConflictFile ], * , progress_callback : Callable [[], None ] = lambda : None , ** conflict2nb_kwargs : Any , ) -> None : \"\"\" Get notebooks from conflicts. Wrap `databooks.conflicts.conflict2nb` to write notebooks to list of `databooks.git_utils.ConflictFile`. :param conflict_files: Files with source conflict files and one-liner git logs :param progress_callback: Callback function to report progress :param conflict2nb_kwargs: Keyword arguments to be passed to `databooks.conflicts.conflict2nb` :return: \"\"\" for conflict in conflict_files : nb = conflict2nb ( conflict , ** conflict2nb_kwargs ) nb . write ( path = conflict . filename , overwrite = True ) progress_callback ()","title":"conflicts2nbs()"},{"location":"conflicts/#databooks.conflicts.path2conflicts","text":"Get the difference model from the path based on the git conflict information. Parameters: Name Type Description Default nb_paths List[Path] Path to file with conflicts (must be notebook paths) required repo Optional[Repo] The git repo to look for conflicts None Returns: Type Description List[ConflictFile] Generator of DiffModel s, to be resolved Source code in databooks/conflicts.py def path2conflicts ( nb_paths : List [ Path ], repo : Optional [ Repo ] = None ) -> List [ ConflictFile ]: \"\"\" Get the difference model from the path based on the git conflict information. :param nb_paths: Path to file with conflicts (must be notebook paths) :param repo: The git repo to look for conflicts :return: Generator of `DiffModel`s, to be resolved \"\"\" if any ( nb_path . suffix not in ( \"\" , \".ipynb\" ) for nb_path in nb_paths ): raise ValueError ( \"Expected either notebook files, a directory or glob expression.\" ) common_parent = find_common_parent ( nb_paths ) repo = get_repo ( common_parent ) if repo is None else repo return [ file for file in get_conflict_blobs ( repo = repo ) if any ( file . filename . match ( str ( p . name )) for p in nb_paths ) ]","title":"path2conflicts()"},{"location":"contrib/","text":"Contributing We are always looking for contributions! You can find below some relevant information and standards for databooks . Setup \u2699\ufe0f After cloning the repo , make sure to set up the environment. Poetry \ud83d\udcdc We use Poetry for both managing environments and packaging. That means you need to install poetry but from there you can use the tool to create the environment. pip install poetry == 1 .1.12 poetry install # installs prod and dev dependencies Usage Remember that to use the environment you can use the poetry run <COMMAND> command or initialize the shell with poetry shell . For example, if you want to create the coverage report you could run poetry run pytest --cov = databooks tests/ or alternatively poetry shell pytest --cov = databooks tests/ Development \ud83d\udee0 We welcome new features, bugfixes or enhancements (whether on code or docs). There are a few standards we adhere to, that are required for new features. Mypy We use type hints! Not only that, they are enforced and checked (with Mypy ). This is actually the reason for supporting Python 3.8+. There are a couple of reasons for using type hints, mainly: Better code coverage (avoid errors during runtime) Improve code understanding As databooks uses both Typer and Pydantic , types are not only for developer hints, but they are also used to cast notebook (JSON) values to the correct types as well as user inputs in the CLI If you are not familiar with type hints and Mypy, a good starting point is watching the Type-checked Python in the real world - PyCon 2018 talk. Linting In regards to code quality, we use a couple of linting tools to maintain the same \"style\" and uphold to the same standards. For that, we use: Black for code formatting isort for imports Flake8 for style enforcement Docs \ud83d\udcda As for documentation, the databooks documentation \"lives\" both on the code itself and supporting documentation (markdown) files. Code Code docs include annotating type hints as well as function docstrings. For that, we use the reStructuredText -like format. Providing docstrings not only give a clear way to document the code, but it is also picked up by MkDocs . MkDocs MkDocs gives a simple way to write markdown files that get rendered as HTML (under a certain theme) and served as documentation. We use MkDocs with different extensions. We use mkdocstrings to link function docstrings with the existing documentation. You can check the generation of documentation by running from the project root mkdocs serve mike We also use the mike plugin in MkDocs to publish and keep different versions of documentation. Cog We use cog to dynamically generate parts of the documentation. That way, code changes trigger markdown changes as well. Pre-commit Pre-commit is the tool that automates everything, eases the workflow and run checks in CI/CD. It's highly recommended installing pre-commit and the hooks during development. Tests \ud83d\uddf3 We use unit tests to ensure that our package works as expected. We use pytest for testing and Pytest-cov for checking how much of the code is covered in our tests. The tests should mimic the package directory structure. The tests are also written to serve as an example of how to use the classes and methods and expected outputs. The coverage is also added to the documentation. For that we use MkDocs Coverage Plugin . For that we need a htmlcov/ directory that is generated by Pytest-cov by running from the project root pytest --cov-report html --cov = databooks tests/ Publishing Publishing is automatically done via Github Actions to PiPy. After published, a new tag and release are created. A new docs version is also published if all previous steps are successful. Contributors \ud83d\udc68\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb databooks was created by Murilo Cunha , and is maintained by dataroots . Acknowledgements Special thanks to: Bart , Nick and Freddy for feedback and support.","title":"Contributing"},{"location":"contrib/#contributing","text":"We are always looking for contributions! You can find below some relevant information and standards for databooks .","title":"Contributing"},{"location":"contrib/#setup","text":"After cloning the repo , make sure to set up the environment.","title":"Setup \u2699\ufe0f"},{"location":"contrib/#poetry","text":"We use Poetry for both managing environments and packaging. That means you need to install poetry but from there you can use the tool to create the environment. pip install poetry == 1 .1.12 poetry install # installs prod and dev dependencies","title":"Poetry \ud83d\udcdc"},{"location":"contrib/#usage","text":"Remember that to use the environment you can use the poetry run <COMMAND> command or initialize the shell with poetry shell . For example, if you want to create the coverage report you could run poetry run pytest --cov = databooks tests/ or alternatively poetry shell pytest --cov = databooks tests/","title":"Usage"},{"location":"contrib/#development","text":"We welcome new features, bugfixes or enhancements (whether on code or docs). There are a few standards we adhere to, that are required for new features.","title":"Development \ud83d\udee0"},{"location":"contrib/#mypy","text":"We use type hints! Not only that, they are enforced and checked (with Mypy ). This is actually the reason for supporting Python 3.8+. There are a couple of reasons for using type hints, mainly: Better code coverage (avoid errors during runtime) Improve code understanding As databooks uses both Typer and Pydantic , types are not only for developer hints, but they are also used to cast notebook (JSON) values to the correct types as well as user inputs in the CLI If you are not familiar with type hints and Mypy, a good starting point is watching the Type-checked Python in the real world - PyCon 2018 talk.","title":"Mypy"},{"location":"contrib/#linting","text":"In regards to code quality, we use a couple of linting tools to maintain the same \"style\" and uphold to the same standards. For that, we use: Black for code formatting isort for imports Flake8 for style enforcement","title":"Linting"},{"location":"contrib/#docs","text":"As for documentation, the databooks documentation \"lives\" both on the code itself and supporting documentation (markdown) files.","title":"Docs \ud83d\udcda"},{"location":"contrib/#code","text":"Code docs include annotating type hints as well as function docstrings. For that, we use the reStructuredText -like format. Providing docstrings not only give a clear way to document the code, but it is also picked up by MkDocs .","title":"Code"},{"location":"contrib/#mkdocs","text":"MkDocs gives a simple way to write markdown files that get rendered as HTML (under a certain theme) and served as documentation. We use MkDocs with different extensions. We use mkdocstrings to link function docstrings with the existing documentation. You can check the generation of documentation by running from the project root mkdocs serve","title":"MkDocs"},{"location":"contrib/#mike","text":"We also use the mike plugin in MkDocs to publish and keep different versions of documentation.","title":"mike"},{"location":"contrib/#cog","text":"We use cog to dynamically generate parts of the documentation. That way, code changes trigger markdown changes as well.","title":"Cog"},{"location":"contrib/#pre-commit","text":"Pre-commit is the tool that automates everything, eases the workflow and run checks in CI/CD. It's highly recommended installing pre-commit and the hooks during development.","title":"Pre-commit"},{"location":"contrib/#tests","text":"We use unit tests to ensure that our package works as expected. We use pytest for testing and Pytest-cov for checking how much of the code is covered in our tests. The tests should mimic the package directory structure. The tests are also written to serve as an example of how to use the classes and methods and expected outputs. The coverage is also added to the documentation. For that we use MkDocs Coverage Plugin . For that we need a htmlcov/ directory that is generated by Pytest-cov by running from the project root pytest --cov-report html --cov = databooks tests/","title":"Tests \ud83d\uddf3"},{"location":"contrib/#publishing","text":"Publishing is automatically done via Github Actions to PiPy. After published, a new tag and release are created. A new docs version is also published if all previous steps are successful.","title":"Publishing"},{"location":"contrib/#contributors","text":"databooks was created by Murilo Cunha , and is maintained by dataroots .","title":"Contributors \ud83d\udc68\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb"},{"location":"contrib/#acknowledgements","text":"Special thanks to: Bart , Nick and Freddy for feedback and support.","title":"Acknowledgements"},{"location":"git_utils/","text":"Git helper functions. ConflictFile dataclass Container for path and different versions of conflicted notebooks. Source code in databooks/git_utils.py class ConflictFile : \"\"\"Container for path and different versions of conflicted notebooks.\"\"\" filename : Path first_log : str last_log : str first_contents : str last_contents : str UnmergedBlob dataclass Container for git unmerged blobs. Source code in databooks/git_utils.py class UnmergedBlob : \"\"\"Container for git unmerged blobs.\"\"\" filename : Path stage : Dict [ int , Blob ] blob2commit ( blob , repo ) Get the short commit message from blob hash. Source code in databooks/git_utils.py def blob2commit ( blob : Blob , repo : Repo ) -> str : \"\"\"Get the short commit message from blob hash.\"\"\" _git = Git ( working_dir = repo . working_dir ) commit_id = _git . log ( find_object = blob , max_count = 1 , all = True , oneline = True ) return ( commit_id if len ( commit_id ) > 0 else _git . stash ( \"list\" , \"--oneline\" , \"--max-count\" , \"1\" , \"--find-object\" , blob ) ) get_conflict_blobs ( repo ) Get the source files for conflicts. Source code in databooks/git_utils.py def get_conflict_blobs ( repo : Repo ) -> List [ ConflictFile ]: \"\"\"Get the source files for conflicts.\"\"\" unmerged_blobs = repo . index . unmerged_blobs () blobs = ( UnmergedBlob ( filename = Path ( k ), stage = dict ( v )) for k , v in unmerged_blobs . items () if 0 not in dict ( v ) . keys () # only get blobs that could not be merged ) if not isinstance ( repo . working_dir , ( Path , str )): raise RuntimeError ( \"Expected `repo` to be `pathlib.Path` or `str`, got\" f \" { type ( repo . working_dir ) } .\" ) return [ ConflictFile ( filename = repo . working_dir / blob . filename , first_log = blob2commit ( blob = blob . stage [ 2 ], repo = repo ), last_log = blob2commit ( blob = blob . stage [ 3 ], repo = repo ), first_contents = repo . git . show ( blob . stage [ 2 ]), last_contents = repo . git . show ( blob . stage [ 3 ]), ) for blob in blobs ] get_repo ( path = PosixPath ( '/home/runner/work/databooks/databooks' )) Find git repo in current or parent directories. Source code in databooks/git_utils.py def get_repo ( path : Path = Path . cwd ()) -> Repo : \"\"\"Find git repo in current or parent directories.\"\"\" repo_dir = find_obj ( obj_name = \".git\" , start = Path ( path . anchor ), finish = path , is_dir = True ) repo = Repo ( path = repo_dir ) logger . debug ( f \"Repo found at: { repo . working_dir } \" ) return repo","title":"Git"},{"location":"git_utils/#databooks.git_utils.ConflictFile","text":"Container for path and different versions of conflicted notebooks. Source code in databooks/git_utils.py class ConflictFile : \"\"\"Container for path and different versions of conflicted notebooks.\"\"\" filename : Path first_log : str last_log : str first_contents : str last_contents : str","title":"ConflictFile"},{"location":"git_utils/#databooks.git_utils.UnmergedBlob","text":"Container for git unmerged blobs. Source code in databooks/git_utils.py class UnmergedBlob : \"\"\"Container for git unmerged blobs.\"\"\" filename : Path stage : Dict [ int , Blob ]","title":"UnmergedBlob"},{"location":"git_utils/#databooks.git_utils.blob2commit","text":"Get the short commit message from blob hash. Source code in databooks/git_utils.py def blob2commit ( blob : Blob , repo : Repo ) -> str : \"\"\"Get the short commit message from blob hash.\"\"\" _git = Git ( working_dir = repo . working_dir ) commit_id = _git . log ( find_object = blob , max_count = 1 , all = True , oneline = True ) return ( commit_id if len ( commit_id ) > 0 else _git . stash ( \"list\" , \"--oneline\" , \"--max-count\" , \"1\" , \"--find-object\" , blob ) )","title":"blob2commit()"},{"location":"git_utils/#databooks.git_utils.get_conflict_blobs","text":"Get the source files for conflicts. Source code in databooks/git_utils.py def get_conflict_blobs ( repo : Repo ) -> List [ ConflictFile ]: \"\"\"Get the source files for conflicts.\"\"\" unmerged_blobs = repo . index . unmerged_blobs () blobs = ( UnmergedBlob ( filename = Path ( k ), stage = dict ( v )) for k , v in unmerged_blobs . items () if 0 not in dict ( v ) . keys () # only get blobs that could not be merged ) if not isinstance ( repo . working_dir , ( Path , str )): raise RuntimeError ( \"Expected `repo` to be `pathlib.Path` or `str`, got\" f \" { type ( repo . working_dir ) } .\" ) return [ ConflictFile ( filename = repo . working_dir / blob . filename , first_log = blob2commit ( blob = blob . stage [ 2 ], repo = repo ), last_log = blob2commit ( blob = blob . stage [ 3 ], repo = repo ), first_contents = repo . git . show ( blob . stage [ 2 ]), last_contents = repo . git . show ( blob . stage [ 3 ]), ) for blob in blobs ]","title":"get_conflict_blobs()"},{"location":"git_utils/#databooks.git_utils.get_repo","text":"Find git repo in current or parent directories. Source code in databooks/git_utils.py def get_repo ( path : Path = Path . cwd ()) -> Repo : \"\"\"Find git repo in current or parent directories.\"\"\" repo_dir = find_obj ( obj_name = \".git\" , start = Path ( path . anchor ), finish = path , is_dir = True ) repo = Repo ( path = repo_dir ) logger . debug ( f \"Repo found at: { repo . working_dir } \" ) return repo","title":"get_repo()"},{"location":"license/","text":"MIT License Copyright (c) 2021 Murilo Kuniyoshi Suzart Cunha Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"logging/","text":"Logging helper functions. get_logger ( name ) Get logger with rich configuration. Source code in databooks/logging.py def get_logger ( name : str ) -> logging . Logger : \"\"\"Get logger with rich configuration.\"\"\" level = os . getenv ( \"LOG_LEVEL\" , logging . INFO ) logging . basicConfig ( level = level , format = \" %(message)s \" , datefmt = \"[ %X ]\" , handlers = [ RichHandler ( rich_tracebacks = True )], ) return logging . getLogger ( name ) set_verbose ( logger ) Set logger to DEBUG level when user requests verbosity. Source code in databooks/logging.py def set_verbose ( logger : logging . Logger ) -> None : \"\"\"Set logger to DEBUG level when user requests verbosity.\"\"\" verbose_level = logging . DEBUG if logger . level < verbose_level : logger . setLevel ( logging . DEBUG ) logger . debug ( f \"Verbose mode: setting log level to { logging . getLevelName ( verbose_level ) } \" )","title":"Logging"},{"location":"logging/#databooks.logging.get_logger","text":"Get logger with rich configuration. Source code in databooks/logging.py def get_logger ( name : str ) -> logging . Logger : \"\"\"Get logger with rich configuration.\"\"\" level = os . getenv ( \"LOG_LEVEL\" , logging . INFO ) logging . basicConfig ( level = level , format = \" %(message)s \" , datefmt = \"[ %X ]\" , handlers = [ RichHandler ( rich_tracebacks = True )], ) return logging . getLogger ( name )","title":"get_logger()"},{"location":"logging/#databooks.logging.set_verbose","text":"Set logger to DEBUG level when user requests verbosity. Source code in databooks/logging.py def set_verbose ( logger : logging . Logger ) -> None : \"\"\"Set logger to DEBUG level when user requests verbosity.\"\"\" verbose_level = logging . DEBUG if logger . level < verbose_level : logger . setLevel ( logging . DEBUG ) logger . debug ( f \"Verbose mode: setting log level to { logging . getLevelName ( verbose_level ) } \" )","title":"set_verbose()"},{"location":"metadata/","text":"Metadata wrapper functions for cleaning notebook metadata. clear ( read_path , write_path = None , notebook_metadata_keep = (), cell_metadata_keep = (), cell_fields_keep = (), check = False , verbose = False , overwrite = False , ** kwargs ) Clear Jupyter Notebook metadata. Clear metadata (at notebook and cell level) and write clean notebook. By default, remove all metadata. Parameters: Name Type Description Default read_path Path Path of notebook file with metadata to be cleaned required write_path Optional[pathlib.Path] Path of notebook file with metadata to be cleaned None notebook_metadata_keep Sequence[str] Notebook metadata fields to keep () cell_metadata_keep Sequence[str] Cell metadata fields to keep () cell_fields_keep Sequence[str] Cell fields to keep () check bool Don't write any files, check whether there is unwanted metadata False verbose bool Log written files False overwrite bool Whether to overwrite files (if exists) False kwargs Any Additional keyword arguments to pass to databooks.data_models.JupyterNotebook.clear_metadata {} Returns: Type Description bool Whether notebooks are equal Source code in databooks/metadata.py def clear ( read_path : Path , write_path : Optional [ Path ] = None , notebook_metadata_keep : Sequence [ str ] = (), cell_metadata_keep : Sequence [ str ] = (), cell_fields_keep : Sequence [ str ] = (), check : bool = False , verbose : bool = False , overwrite : bool = False , ** kwargs : Any , ) -> bool : \"\"\" Clear Jupyter Notebook metadata. Clear metadata (at notebook and cell level) and write clean notebook. By default, remove all metadata. :param read_path: Path of notebook file with metadata to be cleaned :param write_path: Path of notebook file with metadata to be cleaned :param notebook_metadata_keep: Notebook metadata fields to keep :param cell_metadata_keep: Cell metadata fields to keep :param cell_fields_keep: Cell fields to keep :param check: Don't write any files, check whether there is unwanted metadata :param verbose: Log written files :param overwrite: Whether to overwrite files (if exists) :param kwargs: Additional keyword arguments to pass to `databooks.data_models.JupyterNotebook.clear_metadata` :return: Whether notebooks are equal \"\"\" if verbose : set_verbose ( logger ) if write_path is None : write_path = read_path notebook = JupyterNotebook . parse_file ( read_path ) # Get fields to remove from cells and keep notebook schema cell_fields = { field for cell in notebook . cells for field , _ in cell if field } cell_fields_keep = list ( cell_fields_keep ) + list ( BaseCell . __fields__ ) cell_remove_fields = [ field for field in cell_fields if field not in cell_fields_keep ] notebook . clear_metadata ( notebook_metadata_keep = notebook_metadata_keep , cell_metadata_keep = cell_metadata_keep , cell_remove_fields = cell_remove_fields , ** kwargs , ) nb_equals = notebook == JupyterNotebook . parse_file ( read_path ) if nb_equals or check : msg = ( \"only check (unwanted metadata found).\" if not nb_equals else \"no metadata to remove.\" ) logger . debug ( f \"No action taken for { read_path } - \" + msg ) else : notebook . write ( path = write_path , overwrite = overwrite ) logger . debug ( f \"Removed metadata from { read_path } , saved as { write_path } \" ) return nb_equals clear_all ( read_paths , write_paths , * , progress_callback =< function < lambda > at 0x7f6804c04940 > , ** clear_kwargs ) Clear metadata for multiple notebooks at notebooks and cell level. Parameters: Name Type Description Default read_paths List[pathlib.Path] Paths of notebook to remove metadata required write_paths List[pathlib.Path] Paths of where to write cleaned notebooks required progress_callback Callable[[], NoneType] Callback function to report progress <function <lambda> at 0x7f6804c04940> clear_kwargs Any Keyword arguments to be passed to databooks.metadata.clear {} Returns: Type Description List[bool] Whether the notebooks contained or not unwanted metadata Source code in databooks/metadata.py def clear_all ( read_paths : List [ Path ], write_paths : List [ Path ], * , progress_callback : Callable [[], None ] = lambda : None , ** clear_kwargs : Any , ) -> List [ bool ]: \"\"\" Clear metadata for multiple notebooks at notebooks and cell level. :param read_paths: Paths of notebook to remove metadata :param write_paths: Paths of where to write cleaned notebooks :param progress_callback: Callback function to report progress :param clear_kwargs: Keyword arguments to be passed to `databooks.metadata.clear` :return: Whether the notebooks contained or not unwanted metadata \"\"\" if len ( read_paths ) != len ( write_paths ): raise ValueError ( \"Read and write paths must have same length.\" f \" Got { len ( read_paths ) } and { len ( write_paths ) } \" ) checks = [] for nb_path , write_path in zip ( read_paths , write_paths ): checks . append ( clear ( read_path = nb_path , write_path = write_path , ** clear_kwargs )) progress_callback () return checks","title":"Metadata"},{"location":"metadata/#databooks.metadata.clear","text":"Clear Jupyter Notebook metadata. Clear metadata (at notebook and cell level) and write clean notebook. By default, remove all metadata. Parameters: Name Type Description Default read_path Path Path of notebook file with metadata to be cleaned required write_path Optional[pathlib.Path] Path of notebook file with metadata to be cleaned None notebook_metadata_keep Sequence[str] Notebook metadata fields to keep () cell_metadata_keep Sequence[str] Cell metadata fields to keep () cell_fields_keep Sequence[str] Cell fields to keep () check bool Don't write any files, check whether there is unwanted metadata False verbose bool Log written files False overwrite bool Whether to overwrite files (if exists) False kwargs Any Additional keyword arguments to pass to databooks.data_models.JupyterNotebook.clear_metadata {} Returns: Type Description bool Whether notebooks are equal Source code in databooks/metadata.py def clear ( read_path : Path , write_path : Optional [ Path ] = None , notebook_metadata_keep : Sequence [ str ] = (), cell_metadata_keep : Sequence [ str ] = (), cell_fields_keep : Sequence [ str ] = (), check : bool = False , verbose : bool = False , overwrite : bool = False , ** kwargs : Any , ) -> bool : \"\"\" Clear Jupyter Notebook metadata. Clear metadata (at notebook and cell level) and write clean notebook. By default, remove all metadata. :param read_path: Path of notebook file with metadata to be cleaned :param write_path: Path of notebook file with metadata to be cleaned :param notebook_metadata_keep: Notebook metadata fields to keep :param cell_metadata_keep: Cell metadata fields to keep :param cell_fields_keep: Cell fields to keep :param check: Don't write any files, check whether there is unwanted metadata :param verbose: Log written files :param overwrite: Whether to overwrite files (if exists) :param kwargs: Additional keyword arguments to pass to `databooks.data_models.JupyterNotebook.clear_metadata` :return: Whether notebooks are equal \"\"\" if verbose : set_verbose ( logger ) if write_path is None : write_path = read_path notebook = JupyterNotebook . parse_file ( read_path ) # Get fields to remove from cells and keep notebook schema cell_fields = { field for cell in notebook . cells for field , _ in cell if field } cell_fields_keep = list ( cell_fields_keep ) + list ( BaseCell . __fields__ ) cell_remove_fields = [ field for field in cell_fields if field not in cell_fields_keep ] notebook . clear_metadata ( notebook_metadata_keep = notebook_metadata_keep , cell_metadata_keep = cell_metadata_keep , cell_remove_fields = cell_remove_fields , ** kwargs , ) nb_equals = notebook == JupyterNotebook . parse_file ( read_path ) if nb_equals or check : msg = ( \"only check (unwanted metadata found).\" if not nb_equals else \"no metadata to remove.\" ) logger . debug ( f \"No action taken for { read_path } - \" + msg ) else : notebook . write ( path = write_path , overwrite = overwrite ) logger . debug ( f \"Removed metadata from { read_path } , saved as { write_path } \" ) return nb_equals","title":"clear()"},{"location":"metadata/#databooks.metadata.clear_all","text":"Clear metadata for multiple notebooks at notebooks and cell level. Parameters: Name Type Description Default read_paths List[pathlib.Path] Paths of notebook to remove metadata required write_paths List[pathlib.Path] Paths of where to write cleaned notebooks required progress_callback Callable[[], NoneType] Callback function to report progress <function <lambda> at 0x7f6804c04940> clear_kwargs Any Keyword arguments to be passed to databooks.metadata.clear {} Returns: Type Description List[bool] Whether the notebooks contained or not unwanted metadata Source code in databooks/metadata.py def clear_all ( read_paths : List [ Path ], write_paths : List [ Path ], * , progress_callback : Callable [[], None ] = lambda : None , ** clear_kwargs : Any , ) -> List [ bool ]: \"\"\" Clear metadata for multiple notebooks at notebooks and cell level. :param read_paths: Paths of notebook to remove metadata :param write_paths: Paths of where to write cleaned notebooks :param progress_callback: Callback function to report progress :param clear_kwargs: Keyword arguments to be passed to `databooks.metadata.clear` :return: Whether the notebooks contained or not unwanted metadata \"\"\" if len ( read_paths ) != len ( write_paths ): raise ValueError ( \"Read and write paths must have same length.\" f \" Got { len ( read_paths ) } and { len ( write_paths ) } \" ) checks = [] for nb_path , write_path in zip ( read_paths , write_paths ): checks . append ( clear ( read_path = nb_path , write_path = write_path , ** clear_kwargs )) progress_callback () return checks","title":"clear_all()"},{"location":"data_models/base/","text":"Data models - Base Pydantic model with custom methods. BaseCells ( UserList , Generic ) Base abstract class for notebook cells. Source code in databooks/data_models/base.py class BaseCells ( UserList , Generic [ T ]): \"\"\"Base abstract class for notebook cells.\"\"\" @abstractmethod def resolve ( self , ** kwargs : Any ) -> list : \"\"\"Return valid notebook cells from differences.\"\"\" raise NotImplementedError resolve ( self , ** kwargs ) Return valid notebook cells from differences. Source code in databooks/data_models/base.py @abstractmethod def resolve ( self , ** kwargs : Any ) -> list : \"\"\"Return valid notebook cells from differences.\"\"\" raise NotImplementedError DatabooksBase ( BaseModel ) pydantic-model Base Pydantic class with extras on managing fields. Source code in databooks/data_models/base.py class DatabooksBase ( BaseModel ): \"\"\"Base Pydantic class with extras on managing fields.\"\"\" class Config : \"\"\"Default configuration for base class.\"\"\" extra = Extra . allow def remove_fields ( self , fields : Iterable [ str ], * , recursive : bool = False , missing_ok : bool = False , ) -> None : \"\"\" Remove selected fields. :param fields: Fields to remove :param recursive: Whether to remove the fields recursively in case of nested models :param missing_ok: Whether to raise errors in case field is missing :return: \"\"\" d_model = dict ( self ) for field in fields : field_val = d_model . get ( field ) if missing_ok else d_model [ field ] if recursive and isinstance ( field_val , DatabooksBase ): field_val . remove_fields ( fields ) elif field in d_model : delattr ( self , field ) def __str__ ( self ) -> str : \"\"\"Return outputs of __repr__.\"\"\" return repr ( self ) def __sub__ ( self , other : DatabooksBase ) -> DiffModel : \"\"\" Subtraction between `databooks.data_models.base.DatabooksBase` objects. The difference basically return models that replace each fields by a tuple, where for each field we have `field = (self_value, other_value)` \"\"\" if type ( self ) != type ( other ): raise TypeError ( f \"Unsupported operand types for `-`: ` { type ( self ) . __name__ } ` and\" f \" ` { type ( other ) . __name__ } `\" ) # Get field and values for each instance self_d = dict ( self ) other_d = dict ( other ) # Build dict with {field: (type, value)} for each field fields_d : Dict [ str , Any ] = {} for name in self_d . keys () | other_d . keys (): self_val = self_d . get ( name ) other_val = other_d . get ( name ) if type ( self_val ) is type ( other_val ) and all ( isinstance ( val , ( DatabooksBase , BaseCells )) for val in ( self_val , other_val ) ): # Recursively get the diffs for nested models fields_d [ name ] = ( Any , self_val - other_val ) # type: ignore else : fields_d [ name ] = ( tuple , ( self_val , other_val )) # Build Pydantic models dynamically DiffInstance = create_model ( \"Diff\" + type ( self ) . __name__ , __base__ = type ( self ), resolve = resolve , is_diff = True , ** fields_d , ) return cast ( DiffModel , DiffInstance ()) # it'll be filled in with the defaults Config Default configuration for base class. Source code in databooks/data_models/base.py class Config : \"\"\"Default configuration for base class.\"\"\" extra = Extra . allow __str__ ( self ) special Return outputs of repr . Source code in databooks/data_models/base.py def __str__ ( self ) -> str : \"\"\"Return outputs of __repr__.\"\"\" return repr ( self ) __sub__ ( self , other ) special Subtraction between databooks.data_models.base.DatabooksBase objects. The difference basically return models that replace each fields by a tuple, where for each field we have field = (self_value, other_value) Source code in databooks/data_models/base.py def __sub__ ( self , other : DatabooksBase ) -> DiffModel : \"\"\" Subtraction between `databooks.data_models.base.DatabooksBase` objects. The difference basically return models that replace each fields by a tuple, where for each field we have `field = (self_value, other_value)` \"\"\" if type ( self ) != type ( other ): raise TypeError ( f \"Unsupported operand types for `-`: ` { type ( self ) . __name__ } ` and\" f \" ` { type ( other ) . __name__ } `\" ) # Get field and values for each instance self_d = dict ( self ) other_d = dict ( other ) # Build dict with {field: (type, value)} for each field fields_d : Dict [ str , Any ] = {} for name in self_d . keys () | other_d . keys (): self_val = self_d . get ( name ) other_val = other_d . get ( name ) if type ( self_val ) is type ( other_val ) and all ( isinstance ( val , ( DatabooksBase , BaseCells )) for val in ( self_val , other_val ) ): # Recursively get the diffs for nested models fields_d [ name ] = ( Any , self_val - other_val ) # type: ignore else : fields_d [ name ] = ( tuple , ( self_val , other_val )) # Build Pydantic models dynamically DiffInstance = create_model ( \"Diff\" + type ( self ) . __name__ , __base__ = type ( self ), resolve = resolve , is_diff = True , ** fields_d , ) return cast ( DiffModel , DiffInstance ()) # it'll be filled in with the defaults remove_fields ( self , fields , * , recursive = False , missing_ok = False ) Remove selected fields. Parameters: Name Type Description Default fields Iterable[str] Fields to remove required recursive bool Whether to remove the fields recursively in case of nested models False missing_ok bool Whether to raise errors in case field is missing False Returns: Type Description None Source code in databooks/data_models/base.py def remove_fields ( self , fields : Iterable [ str ], * , recursive : bool = False , missing_ok : bool = False , ) -> None : \"\"\" Remove selected fields. :param fields: Fields to remove :param recursive: Whether to remove the fields recursively in case of nested models :param missing_ok: Whether to raise errors in case field is missing :return: \"\"\" d_model = dict ( self ) for field in fields : field_val = d_model . get ( field ) if missing_ok else d_model [ field ] if recursive and isinstance ( field_val , DatabooksBase ): field_val . remove_fields ( fields ) elif field in d_model : delattr ( self , field ) DiffModel ( Protocol , Iterable , Generic ) Protocol for mypy static type checking. Source code in databooks/data_models/base.py class DiffModel ( Protocol , Iterable ): \"\"\"Protocol for mypy static type checking.\"\"\" is_diff : bool def resolve ( self , * args : Any , ** kwargs : Any ) -> DatabooksBase : \"\"\"Protocol method that returns a valid base object.\"\"\" resolve ( self , * args , ** kwargs ) Protocol method that returns a valid base object. Source code in databooks/data_models/base.py def resolve ( self , * args : Any , ** kwargs : Any ) -> DatabooksBase : \"\"\"Protocol method that returns a valid base object.\"\"\" resolve ( model , * , keep_first = True , ignore_none = True , ** kwargs ) Resolve differences for 'diff models'. Return instance alike the parent class databooks.data_models.base.DatabooksBase . Parameters: Name Type Description Default model DiffModel | BaseCells DiffModel that is to be resolved (self when added as a method to a class required keep_first bool Whether to keep the information from the prior in the 'diff model' or the latter True ignore_none bool Whether to ignore None values if encountered, and use the other field value True Returns: Type Description DatabooksBase | List[T] Model with selected fields from the differences Source code in databooks/data_models/base.py def resolve ( model : DiffModel | BaseCells , * , keep_first : bool = True , ignore_none : bool = True , ** kwargs : Any , ) -> DatabooksBase | List [ T ]: \"\"\" Resolve differences for 'diff models'. Return instance alike the parent class `databooks.data_models.base.DatabooksBase`. :param model: DiffModel that is to be resolved (self when added as a method to a class :param keep_first: Whether to keep the information from the prior in the 'diff model' or the latter :param ignore_none: Whether to ignore `None` values if encountered, and use the other field value :return: Model with selected fields from the differences \"\"\" field_d = dict ( model ) is_diff = field_d . pop ( \"is_diff\" ) if not is_diff : raise TypeError ( \"Can only resolve dynamic 'diff models' (when `is_diff=True`).\" ) res_vals : Dict [ str , Any ] = {} for name , value in field_d . items (): if isinstance ( value , ( DiffModel , BaseCells )): res_vals [ name ] = value . resolve ( keep_first = keep_first , ignore_none = ignore_none , ** kwargs ) else : res_vals [ name ] = ( value [ keep_first ] if value [ not keep_first ] is None and ignore_none else value [ not keep_first ] ) return type ( model ) . mro ()[ 1 ]( ** res_vals )","title":"Base"},{"location":"data_models/base/#databooks.data_models.base.BaseCells","text":"Base abstract class for notebook cells. Source code in databooks/data_models/base.py class BaseCells ( UserList , Generic [ T ]): \"\"\"Base abstract class for notebook cells.\"\"\" @abstractmethod def resolve ( self , ** kwargs : Any ) -> list : \"\"\"Return valid notebook cells from differences.\"\"\" raise NotImplementedError","title":"BaseCells"},{"location":"data_models/base/#databooks.data_models.base.BaseCells.resolve","text":"Return valid notebook cells from differences. Source code in databooks/data_models/base.py @abstractmethod def resolve ( self , ** kwargs : Any ) -> list : \"\"\"Return valid notebook cells from differences.\"\"\" raise NotImplementedError","title":"resolve()"},{"location":"data_models/base/#databooks.data_models.base.DatabooksBase","text":"Base Pydantic class with extras on managing fields. Source code in databooks/data_models/base.py class DatabooksBase ( BaseModel ): \"\"\"Base Pydantic class with extras on managing fields.\"\"\" class Config : \"\"\"Default configuration for base class.\"\"\" extra = Extra . allow def remove_fields ( self , fields : Iterable [ str ], * , recursive : bool = False , missing_ok : bool = False , ) -> None : \"\"\" Remove selected fields. :param fields: Fields to remove :param recursive: Whether to remove the fields recursively in case of nested models :param missing_ok: Whether to raise errors in case field is missing :return: \"\"\" d_model = dict ( self ) for field in fields : field_val = d_model . get ( field ) if missing_ok else d_model [ field ] if recursive and isinstance ( field_val , DatabooksBase ): field_val . remove_fields ( fields ) elif field in d_model : delattr ( self , field ) def __str__ ( self ) -> str : \"\"\"Return outputs of __repr__.\"\"\" return repr ( self ) def __sub__ ( self , other : DatabooksBase ) -> DiffModel : \"\"\" Subtraction between `databooks.data_models.base.DatabooksBase` objects. The difference basically return models that replace each fields by a tuple, where for each field we have `field = (self_value, other_value)` \"\"\" if type ( self ) != type ( other ): raise TypeError ( f \"Unsupported operand types for `-`: ` { type ( self ) . __name__ } ` and\" f \" ` { type ( other ) . __name__ } `\" ) # Get field and values for each instance self_d = dict ( self ) other_d = dict ( other ) # Build dict with {field: (type, value)} for each field fields_d : Dict [ str , Any ] = {} for name in self_d . keys () | other_d . keys (): self_val = self_d . get ( name ) other_val = other_d . get ( name ) if type ( self_val ) is type ( other_val ) and all ( isinstance ( val , ( DatabooksBase , BaseCells )) for val in ( self_val , other_val ) ): # Recursively get the diffs for nested models fields_d [ name ] = ( Any , self_val - other_val ) # type: ignore else : fields_d [ name ] = ( tuple , ( self_val , other_val )) # Build Pydantic models dynamically DiffInstance = create_model ( \"Diff\" + type ( self ) . __name__ , __base__ = type ( self ), resolve = resolve , is_diff = True , ** fields_d , ) return cast ( DiffModel , DiffInstance ()) # it'll be filled in with the defaults","title":"DatabooksBase"},{"location":"data_models/base/#databooks.data_models.base.DatabooksBase.Config","text":"Default configuration for base class. Source code in databooks/data_models/base.py class Config : \"\"\"Default configuration for base class.\"\"\" extra = Extra . allow","title":"Config"},{"location":"data_models/base/#databooks.data_models.base.DatabooksBase.__str__","text":"Return outputs of repr . Source code in databooks/data_models/base.py def __str__ ( self ) -> str : \"\"\"Return outputs of __repr__.\"\"\" return repr ( self )","title":"__str__()"},{"location":"data_models/base/#databooks.data_models.base.DatabooksBase.__sub__","text":"Subtraction between databooks.data_models.base.DatabooksBase objects. The difference basically return models that replace each fields by a tuple, where for each field we have field = (self_value, other_value) Source code in databooks/data_models/base.py def __sub__ ( self , other : DatabooksBase ) -> DiffModel : \"\"\" Subtraction between `databooks.data_models.base.DatabooksBase` objects. The difference basically return models that replace each fields by a tuple, where for each field we have `field = (self_value, other_value)` \"\"\" if type ( self ) != type ( other ): raise TypeError ( f \"Unsupported operand types for `-`: ` { type ( self ) . __name__ } ` and\" f \" ` { type ( other ) . __name__ } `\" ) # Get field and values for each instance self_d = dict ( self ) other_d = dict ( other ) # Build dict with {field: (type, value)} for each field fields_d : Dict [ str , Any ] = {} for name in self_d . keys () | other_d . keys (): self_val = self_d . get ( name ) other_val = other_d . get ( name ) if type ( self_val ) is type ( other_val ) and all ( isinstance ( val , ( DatabooksBase , BaseCells )) for val in ( self_val , other_val ) ): # Recursively get the diffs for nested models fields_d [ name ] = ( Any , self_val - other_val ) # type: ignore else : fields_d [ name ] = ( tuple , ( self_val , other_val )) # Build Pydantic models dynamically DiffInstance = create_model ( \"Diff\" + type ( self ) . __name__ , __base__ = type ( self ), resolve = resolve , is_diff = True , ** fields_d , ) return cast ( DiffModel , DiffInstance ()) # it'll be filled in with the defaults","title":"__sub__()"},{"location":"data_models/base/#databooks.data_models.base.DatabooksBase.remove_fields","text":"Remove selected fields. Parameters: Name Type Description Default fields Iterable[str] Fields to remove required recursive bool Whether to remove the fields recursively in case of nested models False missing_ok bool Whether to raise errors in case field is missing False Returns: Type Description None Source code in databooks/data_models/base.py def remove_fields ( self , fields : Iterable [ str ], * , recursive : bool = False , missing_ok : bool = False , ) -> None : \"\"\" Remove selected fields. :param fields: Fields to remove :param recursive: Whether to remove the fields recursively in case of nested models :param missing_ok: Whether to raise errors in case field is missing :return: \"\"\" d_model = dict ( self ) for field in fields : field_val = d_model . get ( field ) if missing_ok else d_model [ field ] if recursive and isinstance ( field_val , DatabooksBase ): field_val . remove_fields ( fields ) elif field in d_model : delattr ( self , field )","title":"remove_fields()"},{"location":"data_models/base/#databooks.data_models.base.DiffModel","text":"Protocol for mypy static type checking. Source code in databooks/data_models/base.py class DiffModel ( Protocol , Iterable ): \"\"\"Protocol for mypy static type checking.\"\"\" is_diff : bool def resolve ( self , * args : Any , ** kwargs : Any ) -> DatabooksBase : \"\"\"Protocol method that returns a valid base object.\"\"\"","title":"DiffModel"},{"location":"data_models/base/#databooks.data_models.base.DiffModel.resolve","text":"Protocol method that returns a valid base object. Source code in databooks/data_models/base.py def resolve ( self , * args : Any , ** kwargs : Any ) -> DatabooksBase : \"\"\"Protocol method that returns a valid base object.\"\"\"","title":"resolve()"},{"location":"data_models/base/#databooks.data_models.base.resolve","text":"Resolve differences for 'diff models'. Return instance alike the parent class databooks.data_models.base.DatabooksBase . Parameters: Name Type Description Default model DiffModel | BaseCells DiffModel that is to be resolved (self when added as a method to a class required keep_first bool Whether to keep the information from the prior in the 'diff model' or the latter True ignore_none bool Whether to ignore None values if encountered, and use the other field value True Returns: Type Description DatabooksBase | List[T] Model with selected fields from the differences Source code in databooks/data_models/base.py def resolve ( model : DiffModel | BaseCells , * , keep_first : bool = True , ignore_none : bool = True , ** kwargs : Any , ) -> DatabooksBase | List [ T ]: \"\"\" Resolve differences for 'diff models'. Return instance alike the parent class `databooks.data_models.base.DatabooksBase`. :param model: DiffModel that is to be resolved (self when added as a method to a class :param keep_first: Whether to keep the information from the prior in the 'diff model' or the latter :param ignore_none: Whether to ignore `None` values if encountered, and use the other field value :return: Model with selected fields from the differences \"\"\" field_d = dict ( model ) is_diff = field_d . pop ( \"is_diff\" ) if not is_diff : raise TypeError ( \"Can only resolve dynamic 'diff models' (when `is_diff=True`).\" ) res_vals : Dict [ str , Any ] = {} for name , value in field_d . items (): if isinstance ( value , ( DiffModel , BaseCells )): res_vals [ name ] = value . resolve ( keep_first = keep_first , ignore_none = ignore_none , ** kwargs ) else : res_vals [ name ] = ( value [ keep_first ] if value [ not keep_first ] is None and ignore_none else value [ not keep_first ] ) return type ( model ) . mro ()[ 1 ]( ** res_vals )","title":"resolve()"},{"location":"data_models/notebook/","text":"Data models - Jupyter Notebooks and components. Cells ( GenericModel , BaseCells ) pydantic-model Similar to list , with - operator using difflib.SequenceMatcher . Source code in databooks/data_models/notebook.py class Cells ( GenericModel , BaseCells [ T ]): \"\"\"Similar to `list`, with `-` operator using `difflib.SequenceMatcher`.\"\"\" __root__ : Sequence [ T ] = () def __init__ ( self , elements : Sequence [ T ] = ()) -> None : \"\"\"Allow passing data as a positional argument when instantiating class.\"\"\" super ( Cells , self ) . __init__ ( __root__ = elements ) @property def data ( self ) -> List [ T ]: # type: ignore \"\"\"Define property `data` required for `collections.UserList` class.\"\"\" return list ( self . __root__ ) def __iter__ ( self ) -> Generator [ Any , None , None ]: \"\"\"Use list property as iterable.\"\"\" return ( el for el in self . data ) def __sub__ ( self : Cells [ Cell ], other : Cells [ Cell ]) -> Cells [ CellsPair ]: \"\"\"Return the difference using `difflib.SequenceMatcher`.\"\"\" if type ( self ) != type ( other ): raise TypeError ( f \"Unsupported operand types for `-`: ` { type ( self ) . __name__ } ` and\" f \" ` { type ( other ) . __name__ } `\" ) # By setting the context to the max number of cells and using # `pathlib.SequenceMatcher.get_grouped_opcodes` we essentially get the same # result as `pathlib.SequenceMatcher.get_opcodes` but in smaller chunks n_context = max ( len ( self ), len ( other )) diff_opcodes = list ( SequenceMatcher ( isjunk = None , a = self , b = other , autojunk = False ) . get_grouped_opcodes ( n_context ) ) if len ( diff_opcodes ) > 1 : raise RuntimeError ( \"Expected one group for opcodes when context size is \" f \" { n_context } for { len ( self ) } and { len ( other ) } cells in\" \" notebooks.\" ) return Cells [ CellsPair ]( [ # https://github.com/python/mypy/issues/9459 tuple (( self . data [ i1 : j1 ], other . data [ i2 : j2 ])) # type: ignore for _ , i1 , j1 , i2 , j2 in chain . from_iterable ( diff_opcodes ) ] ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : \"\"\"Rich display of all cells in notebook.\"\"\" yield from self . _get_renderables ( expand = True , width = options . max_width // 3 ) def _get_renderables ( self , ** wrap_cols_kwargs : Any ) -> Iterable [ RenderableType ]: \"\"\"Get the Rich renderables, depending on whether `Cells` is a diff or not.\"\"\" if all ( isinstance ( el , tuple ) for el in self . data ): return chain . from_iterable ( Cells . wrap_cols ( val [ 0 ], val [ 1 ], ** wrap_cols_kwargs ) if val [ 0 ] != val [ 1 ] else val [ 0 ] for val in cast ( List [ CellsPair ], self . data ) ) return cast ( List [ Cell ], self . data ) @classmethod def __get_validators__ ( cls ) -> Generator [ Callable [ ... , Any ], None , None ]: \"\"\"Get validators for custom class.\"\"\" yield cls . validate @classmethod def validate ( cls , v : List [ T ]) -> Cells [ T ]: \"\"\"Ensure object is custom defined container.\"\"\" if not isinstance ( v , cls ): return cls ( v ) else : return v @classmethod def wrap_cols ( cls , first_cells : List [ Cell ], last_cells : List [ Cell ], ** cols_kwargs : Any ) -> Sequence [ Columns ]: \"\"\"Wrap the first and second cells into colunmns for iterable.\"\"\" _empty = [ Panel ( Text ( \"<None>\" , justify = \"center\" ), box = box . SIMPLE )] _first = Group ( * first_cells or _empty ) _last = Group ( * last_cells or _empty ) return [ Columns ([ _first , _last ], ** cols_kwargs )] @staticmethod def wrap_git ( first_cells : List [ Cell ], last_cells : List [ Cell ], hash_first : Optional [ str ] = None , hash_last : Optional [ str ] = None , ) -> Sequence [ Cell ]: \"\"\"Wrap git-diff cells in existing notebook.\"\"\" return [ MarkdownCell ( metadata = CellMetadata ( git_hash = hash_first ), source = [ f \"`<<<<<<< { hash_first } `\" ], cell_type = \"markdown\" , ), * first_cells , MarkdownCell ( source = [ \"`=======`\" ], cell_type = \"markdown\" , metadata = CellMetadata (), ), * last_cells , MarkdownCell ( metadata = CellMetadata ( git_hash = hash_last ), source = [ f \"`>>>>>>> { hash_last } `\" ], cell_type = \"markdown\" , ), ] def resolve ( self : Cells [ CellsPair ], * , keep_first_cells : Optional [ bool ] = None , first_id : Optional [ str ] = None , last_id : Optional [ str ] = None , ** kwargs : Any , ) -> List [ Cell ]: \"\"\" Resolve differences between `databooks.data_models.notebook.Cells`. :param keep_first_cells: Whether to keep the cells of the first notebook or not. If `None`, then keep both wrapping the git-diff tags :param first_id: Git hash of first file in conflict :param last_id: Git hash of last file in conflict :param kwargs: (Unused) keyword arguments to keep compatibility with `databooks.data_models.base.resolve` :return: List of cells \"\"\" if keep_first_cells is not None : return list ( chain . from_iterable ( pairs [ not keep_first_cells ] for pairs in self . data ) ) return list ( chain . from_iterable ( Cells . wrap_git ( first_cells = val [ 0 ], last_cells = val [ 1 ], hash_first = first_id , hash_last = last_id , ) if val [ 0 ] != val [ 1 ] else val [ 0 ] for val in self . data ) ) data : List [ T ] property readonly Define property data required for collections.UserList class. __get_validators__ () classmethod special Get validators for custom class. Source code in databooks/data_models/notebook.py @classmethod def __get_validators__ ( cls ) -> Generator [ Callable [ ... , Any ], None , None ]: \"\"\"Get validators for custom class.\"\"\" yield cls . validate __init__ ( self , elements = ()) special Allow passing data as a positional argument when instantiating class. Source code in databooks/data_models/notebook.py def __init__ ( self , elements : Sequence [ T ] = ()) -> None : \"\"\"Allow passing data as a positional argument when instantiating class.\"\"\" super ( Cells , self ) . __init__ ( __root__ = elements ) __iter__ ( self ) special Use list property as iterable. Source code in databooks/data_models/notebook.py def __iter__ ( self ) -> Generator [ Any , None , None ]: \"\"\"Use list property as iterable.\"\"\" return ( el for el in self . data ) __rich_console__ ( self , console , options ) special Rich display of all cells in notebook. Source code in databooks/data_models/notebook.py def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : \"\"\"Rich display of all cells in notebook.\"\"\" yield from self . _get_renderables ( expand = True , width = options . max_width // 3 ) __sub__ ( self , other ) special Return the difference using difflib.SequenceMatcher . Source code in databooks/data_models/notebook.py def __sub__ ( self : Cells [ Cell ], other : Cells [ Cell ]) -> Cells [ CellsPair ]: \"\"\"Return the difference using `difflib.SequenceMatcher`.\"\"\" if type ( self ) != type ( other ): raise TypeError ( f \"Unsupported operand types for `-`: ` { type ( self ) . __name__ } ` and\" f \" ` { type ( other ) . __name__ } `\" ) # By setting the context to the max number of cells and using # `pathlib.SequenceMatcher.get_grouped_opcodes` we essentially get the same # result as `pathlib.SequenceMatcher.get_opcodes` but in smaller chunks n_context = max ( len ( self ), len ( other )) diff_opcodes = list ( SequenceMatcher ( isjunk = None , a = self , b = other , autojunk = False ) . get_grouped_opcodes ( n_context ) ) if len ( diff_opcodes ) > 1 : raise RuntimeError ( \"Expected one group for opcodes when context size is \" f \" { n_context } for { len ( self ) } and { len ( other ) } cells in\" \" notebooks.\" ) return Cells [ CellsPair ]( [ # https://github.com/python/mypy/issues/9459 tuple (( self . data [ i1 : j1 ], other . data [ i2 : j2 ])) # type: ignore for _ , i1 , j1 , i2 , j2 in chain . from_iterable ( diff_opcodes ) ] ) resolve ( self , * , keep_first_cells = None , first_id = None , last_id = None , ** kwargs ) Resolve differences between databooks.data_models.notebook.Cells . Parameters: Name Type Description Default keep_first_cells Optional[bool] Whether to keep the cells of the first notebook or not. If None , then keep both wrapping the git-diff tags None first_id Optional[str] Git hash of first file in conflict None last_id Optional[str] Git hash of last file in conflict None kwargs Any (Unused) keyword arguments to keep compatibility with databooks.data_models.base.resolve {} Returns: Type Description List[Cell] List of cells Source code in databooks/data_models/notebook.py def resolve ( self : Cells [ CellsPair ], * , keep_first_cells : Optional [ bool ] = None , first_id : Optional [ str ] = None , last_id : Optional [ str ] = None , ** kwargs : Any , ) -> List [ Cell ]: \"\"\" Resolve differences between `databooks.data_models.notebook.Cells`. :param keep_first_cells: Whether to keep the cells of the first notebook or not. If `None`, then keep both wrapping the git-diff tags :param first_id: Git hash of first file in conflict :param last_id: Git hash of last file in conflict :param kwargs: (Unused) keyword arguments to keep compatibility with `databooks.data_models.base.resolve` :return: List of cells \"\"\" if keep_first_cells is not None : return list ( chain . from_iterable ( pairs [ not keep_first_cells ] for pairs in self . data ) ) return list ( chain . from_iterable ( Cells . wrap_git ( first_cells = val [ 0 ], last_cells = val [ 1 ], hash_first = first_id , hash_last = last_id , ) if val [ 0 ] != val [ 1 ] else val [ 0 ] for val in self . data ) ) validate ( v ) classmethod Ensure object is custom defined container. Source code in databooks/data_models/notebook.py @classmethod def validate ( cls , v : List [ T ]) -> Cells [ T ]: \"\"\"Ensure object is custom defined container.\"\"\" if not isinstance ( v , cls ): return cls ( v ) else : return v wrap_cols ( first_cells , last_cells , ** cols_kwargs ) classmethod Wrap the first and second cells into colunmns for iterable. Source code in databooks/data_models/notebook.py @classmethod def wrap_cols ( cls , first_cells : List [ Cell ], last_cells : List [ Cell ], ** cols_kwargs : Any ) -> Sequence [ Columns ]: \"\"\"Wrap the first and second cells into colunmns for iterable.\"\"\" _empty = [ Panel ( Text ( \"<None>\" , justify = \"center\" ), box = box . SIMPLE )] _first = Group ( * first_cells or _empty ) _last = Group ( * last_cells or _empty ) return [ Columns ([ _first , _last ], ** cols_kwargs )] wrap_git ( first_cells , last_cells , hash_first = None , hash_last = None ) staticmethod Wrap git-diff cells in existing notebook. Source code in databooks/data_models/notebook.py @staticmethod def wrap_git ( first_cells : List [ Cell ], last_cells : List [ Cell ], hash_first : Optional [ str ] = None , hash_last : Optional [ str ] = None , ) -> Sequence [ Cell ]: \"\"\"Wrap git-diff cells in existing notebook.\"\"\" return [ MarkdownCell ( metadata = CellMetadata ( git_hash = hash_first ), source = [ f \"`<<<<<<< { hash_first } `\" ], cell_type = \"markdown\" , ), * first_cells , MarkdownCell ( source = [ \"`=======`\" ], cell_type = \"markdown\" , metadata = CellMetadata (), ), * last_cells , MarkdownCell ( metadata = CellMetadata ( git_hash = hash_last ), source = [ f \"`>>>>>>> { hash_last } `\" ], cell_type = \"markdown\" , ), ] Cells[Union[CodeCell, RawCell, MarkdownCell]] ( Cells ) pydantic-model Config getter_dict ( Representation ) Hack to make object's smell just enough like dicts for validate_model. We can't inherit from Mapping[str, Any] because it upsets cython so we have to implement all methods ourselves. get_field_info ( name ) classmethod Get properties of FieldInfo from the fields property of the config class. json_dumps ( obj , * , skipkeys = False , ensure_ascii = True , check_circular = True , allow_nan = True , cls = None , indent = None , separators = None , default = None , sort_keys = False , ** kw ) Serialize obj to a JSON formatted str . If skipkeys is true then dict keys that are not basic types ( str , int , float , bool , None ) will be skipped instead of raising a TypeError . If ensure_ascii is false, then the return value can contain non-ASCII characters if they appear in strings contained in obj . Otherwise, all such characters are escaped in JSON strings. If check_circular is false, then the circular reference check for container types will be skipped and a circular reference will result in an OverflowError (or worse). If allow_nan is false, then it will be a ValueError to serialize out of range float values ( nan , inf , -inf ) in strict compliance of the JSON specification, instead of using the JavaScript equivalents ( NaN , Infinity , -Infinity ). If indent is a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. If specified, separators should be an (item_separator, key_separator) tuple. The default is (', ', ': ') if indent is None and (',', ': ') otherwise. To get the most compact JSON representation, you should specify (',', ':') to eliminate whitespace. default(obj) is a function that should return a serializable version of obj or raise TypeError. The default simply raises TypeError. If sort_keys is true (default: False ), then the output of dictionaries will be sorted by key. To use a custom JSONEncoder subclass (e.g. one that overrides the .default() method to serialize additional types), specify it with the cls kwarg; otherwise JSONEncoder is used. Source code in databooks/data_models/notebook.py def dumps ( obj , * , skipkeys = False , ensure_ascii = True , check_circular = True , allow_nan = True , cls = None , indent = None , separators = None , default = None , sort_keys = False , ** kw ): \"\"\"Serialize ``obj`` to a JSON formatted ``str``. If ``skipkeys`` is true then ``dict`` keys that are not basic types (``str``, ``int``, ``float``, ``bool``, ``None``) will be skipped instead of raising a ``TypeError``. If ``ensure_ascii`` is false, then the return value can contain non-ASCII characters if they appear in strings contained in ``obj``. Otherwise, all such characters are escaped in JSON strings. If ``check_circular`` is false, then the circular reference check for container types will be skipped and a circular reference will result in an ``OverflowError`` (or worse). If ``allow_nan`` is false, then it will be a ``ValueError`` to serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) in strict compliance of the JSON specification, instead of using the JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``). If ``indent`` is a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact representation. If specified, ``separators`` should be an ``(item_separator, key_separator)`` tuple. The default is ``(', ', ': ')`` if *indent* is ``None`` and ``(',', ': ')`` otherwise. To get the most compact JSON representation, you should specify ``(',', ':')`` to eliminate whitespace. ``default(obj)`` is a function that should return a serializable version of obj or raise TypeError. The default simply raises TypeError. If *sort_keys* is true (default: ``False``), then the output of dictionaries will be sorted by key. To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the ``.default()`` method to serialize additional types), specify it with the ``cls`` kwarg; otherwise ``JSONEncoder`` is used. \"\"\" # cached encoder if ( not skipkeys and ensure_ascii and check_circular and allow_nan and cls is None and indent is None and separators is None and default is None and not sort_keys and not kw ): return _default_encoder . encode ( obj ) if cls is None : cls = JSONEncoder return cls ( skipkeys = skipkeys , ensure_ascii = ensure_ascii , check_circular = check_circular , allow_nan = allow_nan , indent = indent , separators = separators , default = default , sort_keys = sort_keys , ** kw ) . encode ( obj ) json_loads ( s , * , cls = None , object_hook = None , parse_float = None , parse_int = None , parse_constant = None , object_pairs_hook = None , ** kw ) Deserialize s (a str , bytes or bytearray instance containing a JSON document) to a Python object. object_hook is an optional function that will be called with the result of any object literal decode (a dict ). The return value of object_hook will be used instead of the dict . This feature can be used to implement custom decoders (e.g. JSON-RPC class hinting). object_pairs_hook is an optional function that will be called with the result of any object literal decoded with an ordered list of pairs. The return value of object_pairs_hook will be used instead of the dict . This feature can be used to implement custom decoders. If object_hook is also defined, the object_pairs_hook takes priority. parse_float , if specified, will be called with the string of every JSON float to be decoded. By default this is equivalent to float(num_str). This can be used to use another datatype or parser for JSON floats (e.g. decimal.Decimal). parse_int , if specified, will be called with the string of every JSON int to be decoded. By default this is equivalent to int(num_str). This can be used to use another datatype or parser for JSON integers (e.g. float). parse_constant , if specified, will be called with one of the following strings: -Infinity, Infinity, NaN. This can be used to raise an exception if invalid JSON numbers are encountered. To use a custom JSONDecoder subclass, specify it with the cls kwarg; otherwise JSONDecoder is used. The encoding argument is ignored and deprecated since Python 3.1. Source code in databooks/data_models/notebook.py def loads ( s , * , cls = None , object_hook = None , parse_float = None , parse_int = None , parse_constant = None , object_pairs_hook = None , ** kw ): \"\"\"Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance containing a JSON document) to a Python object. ``object_hook`` is an optional function that will be called with the result of any object literal decode (a ``dict``). The return value of ``object_hook`` will be used instead of the ``dict``. This feature can be used to implement custom decoders (e.g. JSON-RPC class hinting). ``object_pairs_hook`` is an optional function that will be called with the result of any object literal decoded with an ordered list of pairs. The return value of ``object_pairs_hook`` will be used instead of the ``dict``. This feature can be used to implement custom decoders. If ``object_hook`` is also defined, the ``object_pairs_hook`` takes priority. ``parse_float``, if specified, will be called with the string of every JSON float to be decoded. By default this is equivalent to float(num_str). This can be used to use another datatype or parser for JSON floats (e.g. decimal.Decimal). ``parse_int``, if specified, will be called with the string of every JSON int to be decoded. By default this is equivalent to int(num_str). This can be used to use another datatype or parser for JSON integers (e.g. float). ``parse_constant``, if specified, will be called with one of the following strings: -Infinity, Infinity, NaN. This can be used to raise an exception if invalid JSON numbers are encountered. To use a custom ``JSONDecoder`` subclass, specify it with the ``cls`` kwarg; otherwise ``JSONDecoder`` is used. The ``encoding`` argument is ignored and deprecated since Python 3.1. \"\"\" if isinstance ( s , str ): if s . startswith ( ' \\ufeff ' ): raise JSONDecodeError ( \"Unexpected UTF-8 BOM (decode using utf-8-sig)\" , s , 0 ) else : if not isinstance ( s , ( bytes , bytearray )): raise TypeError ( f 'the JSON object must be str, bytes or bytearray, ' f 'not { s . __class__ . __name__ } ' ) s = s . decode ( detect_encoding ( s ), 'surrogatepass' ) if \"encoding\" in kw : import warnings warnings . warn ( \"'encoding' is ignored and deprecated. It will be removed in Python 3.9\" , DeprecationWarning , stacklevel = 2 ) del kw [ 'encoding' ] if ( cls is None and object_hook is None and parse_int is None and parse_float is None and parse_constant is None and object_pairs_hook is None and not kw ): return _default_decoder . decode ( s ) if cls is None : cls = JSONDecoder if object_hook is not None : kw [ 'object_hook' ] = object_hook if object_pairs_hook is not None : kw [ 'object_pairs_hook' ] = object_pairs_hook if parse_float is not None : kw [ 'parse_float' ] = parse_float if parse_int is not None : kw [ 'parse_int' ] = parse_int if parse_constant is not None : kw [ 'parse_constant' ] = parse_constant return cls ( ** kw ) . decode ( s ) prepare_field ( field ) classmethod Optional hook to check or modify fields during model creation. JupyterNotebook ( DatabooksBase ) pydantic-model Jupyter notebook. Extra fields yield invalid notebook. Source code in databooks/data_models/notebook.py class JupyterNotebook ( DatabooksBase , extra = Extra . forbid ): \"\"\"Jupyter notebook. Extra fields yield invalid notebook.\"\"\" nbformat : int nbformat_minor : int metadata : NotebookMetadata cells : Cells [ Cell ] def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : \"\"\"Rich display notebook.\"\"\" def _rich ( kernel : str ) -> Text : \"\"\"Display with `kernel` theme, horizontal padding and right-justified.\"\"\" return Text ( kernel , style = \"kernel\" , justify = \"right\" ) kernelspec = self . metadata . dict () . get ( \"kernelspec\" , {}) if isinstance ( kernelspec , tuple ): # check if this is a `DiffCells` lang_first , lang_last = ( ks . get ( \"language\" , \"text\" ) for ks in kernelspec ) nb_lang = lang_first if lang_first == lang_last else \"text\" if any ( \"display_name\" in ks . keys () for ks in kernelspec ): kernel_first , kernel_last = [ _rich ( ks [ \"display_name\" ]) for ks in kernelspec ] yield Columns ( [ kernel_first , kernel_last ], expand = True , width = options . max_width // 3 , ) if kernel_first != kernel_last else kernel_first else : nb_lang = kernelspec . get ( \"language\" , \"text\" ) if \"display_name\" in kernelspec . keys (): yield _rich ( kernelspec [ \"display_name\" ]) for cell in self . cells : if isinstance ( cell , CodeCell ): cell . metadata = CellMetadata ( ** cell . metadata . dict (), lang = nb_lang ) yield self . cells @classmethod def parse_file ( cls , path : Path | str , ** parse_kwargs : Any ) -> JupyterNotebook : \"\"\"Parse notebook from a path.\"\"\" content_arg = parse_kwargs . pop ( \"content_type\" , None ) if content_arg is not None : raise ValueError ( f \"Value of `content_type` must be `json` (default), got ` { content_arg } `\" ) return super ( JupyterNotebook , cls ) . parse_file ( path = path , content_type = \"json\" , ** parse_kwargs ) def write ( self , path : Path | str , overwrite : bool = False , ** json_kwargs : Any ) -> None : \"\"\"Write notebook to disk.\"\"\" path = Path ( path ) if not isinstance ( path , Path ) else path json_kwargs = { \"indent\" : 2 , ** json_kwargs } if path . is_file () and not overwrite : raise ValueError ( f \"File exists at { path } exists. Specify `overwrite = True`.\" ) _ , _ , validation_error = validate_model ( self . __class__ , self . dict ()) if validation_error : raise validation_error with path . open ( \"w\" ) as f : json . dump ( self . dict (), fp = f , ** json_kwargs ) def clear_metadata ( self , * , notebook_metadata_keep : Sequence [ str ] = None , notebook_metadata_remove : Sequence [ str ] = None , ** cell_kwargs : Any , ) -> None : \"\"\" Clear notebook and cell metadata. :param notebook_metadata_keep: Metadata values to keep - simply pass an empty sequence (i.e.: `()`) to remove all extra fields. :param notebook_metadata_remove: Metadata values to remove :param cell_kwargs: keyword arguments to be passed to each cell's `databooks.data_models.cell.BaseCell.clear_metadata` :return: \"\"\" nargs = sum ( ( notebook_metadata_keep is not None , notebook_metadata_remove is not None ) ) if nargs != 1 : raise ValueError ( \"Exactly one of `notebook_metadata_keep` or `notebook_metadata_remove`\" f \" must be passed, got { nargs } arguments.\" ) if notebook_metadata_keep is not None : notebook_metadata_remove = tuple ( field for field , _ in self . metadata if field not in notebook_metadata_keep ) self . metadata . remove_fields ( notebook_metadata_remove ) # type: ignore if len ( cell_kwargs ) > 0 : _clean_cells = deepcopy ( self . cells ) for cell in _clean_cells : cell . clear_fields ( ** cell_kwargs ) self . cells = _clean_cells __rich_console__ ( self , console , options ) special Rich display notebook. Source code in databooks/data_models/notebook.py def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : \"\"\"Rich display notebook.\"\"\" def _rich ( kernel : str ) -> Text : \"\"\"Display with `kernel` theme, horizontal padding and right-justified.\"\"\" return Text ( kernel , style = \"kernel\" , justify = \"right\" ) kernelspec = self . metadata . dict () . get ( \"kernelspec\" , {}) if isinstance ( kernelspec , tuple ): # check if this is a `DiffCells` lang_first , lang_last = ( ks . get ( \"language\" , \"text\" ) for ks in kernelspec ) nb_lang = lang_first if lang_first == lang_last else \"text\" if any ( \"display_name\" in ks . keys () for ks in kernelspec ): kernel_first , kernel_last = [ _rich ( ks [ \"display_name\" ]) for ks in kernelspec ] yield Columns ( [ kernel_first , kernel_last ], expand = True , width = options . max_width // 3 , ) if kernel_first != kernel_last else kernel_first else : nb_lang = kernelspec . get ( \"language\" , \"text\" ) if \"display_name\" in kernelspec . keys (): yield _rich ( kernelspec [ \"display_name\" ]) for cell in self . cells : if isinstance ( cell , CodeCell ): cell . metadata = CellMetadata ( ** cell . metadata . dict (), lang = nb_lang ) yield self . cells clear_metadata ( self , * , notebook_metadata_keep = None , notebook_metadata_remove = None , ** cell_kwargs ) Clear notebook and cell metadata. Parameters: Name Type Description Default notebook_metadata_keep Sequence[str] Metadata values to keep - simply pass an empty sequence (i.e.: () ) to remove all extra fields. None notebook_metadata_remove Sequence[str] Metadata values to remove None cell_kwargs Any keyword arguments to be passed to each cell's databooks.data_models.cell.BaseCell.clear_metadata {} Returns: Type Description None Source code in databooks/data_models/notebook.py def clear_metadata ( self , * , notebook_metadata_keep : Sequence [ str ] = None , notebook_metadata_remove : Sequence [ str ] = None , ** cell_kwargs : Any , ) -> None : \"\"\" Clear notebook and cell metadata. :param notebook_metadata_keep: Metadata values to keep - simply pass an empty sequence (i.e.: `()`) to remove all extra fields. :param notebook_metadata_remove: Metadata values to remove :param cell_kwargs: keyword arguments to be passed to each cell's `databooks.data_models.cell.BaseCell.clear_metadata` :return: \"\"\" nargs = sum ( ( notebook_metadata_keep is not None , notebook_metadata_remove is not None ) ) if nargs != 1 : raise ValueError ( \"Exactly one of `notebook_metadata_keep` or `notebook_metadata_remove`\" f \" must be passed, got { nargs } arguments.\" ) if notebook_metadata_keep is not None : notebook_metadata_remove = tuple ( field for field , _ in self . metadata if field not in notebook_metadata_keep ) self . metadata . remove_fields ( notebook_metadata_remove ) # type: ignore if len ( cell_kwargs ) > 0 : _clean_cells = deepcopy ( self . cells ) for cell in _clean_cells : cell . clear_fields ( ** cell_kwargs ) self . cells = _clean_cells parse_file ( path , ** parse_kwargs ) classmethod Parse notebook from a path. Source code in databooks/data_models/notebook.py @classmethod def parse_file ( cls , path : Path | str , ** parse_kwargs : Any ) -> JupyterNotebook : \"\"\"Parse notebook from a path.\"\"\" content_arg = parse_kwargs . pop ( \"content_type\" , None ) if content_arg is not None : raise ValueError ( f \"Value of `content_type` must be `json` (default), got ` { content_arg } `\" ) return super ( JupyterNotebook , cls ) . parse_file ( path = path , content_type = \"json\" , ** parse_kwargs ) write ( self , path , overwrite = False , ** json_kwargs ) Write notebook to disk. Source code in databooks/data_models/notebook.py def write ( self , path : Path | str , overwrite : bool = False , ** json_kwargs : Any ) -> None : \"\"\"Write notebook to disk.\"\"\" path = Path ( path ) if not isinstance ( path , Path ) else path json_kwargs = { \"indent\" : 2 , ** json_kwargs } if path . is_file () and not overwrite : raise ValueError ( f \"File exists at { path } exists. Specify `overwrite = True`.\" ) _ , _ , validation_error = validate_model ( self . __class__ , self . dict ()) if validation_error : raise validation_error with path . open ( \"w\" ) as f : json . dump ( self . dict (), fp = f , ** json_kwargs ) NotebookMetadata ( DatabooksBase ) pydantic-model Notebook metadata. Empty by default but can accept extra fields. Source code in databooks/data_models/notebook.py class NotebookMetadata ( DatabooksBase ): \"\"\"Notebook metadata. Empty by default but can accept extra fields.\"\"\"","title":"Notebooks"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells","text":"Similar to list , with - operator using difflib.SequenceMatcher . Source code in databooks/data_models/notebook.py class Cells ( GenericModel , BaseCells [ T ]): \"\"\"Similar to `list`, with `-` operator using `difflib.SequenceMatcher`.\"\"\" __root__ : Sequence [ T ] = () def __init__ ( self , elements : Sequence [ T ] = ()) -> None : \"\"\"Allow passing data as a positional argument when instantiating class.\"\"\" super ( Cells , self ) . __init__ ( __root__ = elements ) @property def data ( self ) -> List [ T ]: # type: ignore \"\"\"Define property `data` required for `collections.UserList` class.\"\"\" return list ( self . __root__ ) def __iter__ ( self ) -> Generator [ Any , None , None ]: \"\"\"Use list property as iterable.\"\"\" return ( el for el in self . data ) def __sub__ ( self : Cells [ Cell ], other : Cells [ Cell ]) -> Cells [ CellsPair ]: \"\"\"Return the difference using `difflib.SequenceMatcher`.\"\"\" if type ( self ) != type ( other ): raise TypeError ( f \"Unsupported operand types for `-`: ` { type ( self ) . __name__ } ` and\" f \" ` { type ( other ) . __name__ } `\" ) # By setting the context to the max number of cells and using # `pathlib.SequenceMatcher.get_grouped_opcodes` we essentially get the same # result as `pathlib.SequenceMatcher.get_opcodes` but in smaller chunks n_context = max ( len ( self ), len ( other )) diff_opcodes = list ( SequenceMatcher ( isjunk = None , a = self , b = other , autojunk = False ) . get_grouped_opcodes ( n_context ) ) if len ( diff_opcodes ) > 1 : raise RuntimeError ( \"Expected one group for opcodes when context size is \" f \" { n_context } for { len ( self ) } and { len ( other ) } cells in\" \" notebooks.\" ) return Cells [ CellsPair ]( [ # https://github.com/python/mypy/issues/9459 tuple (( self . data [ i1 : j1 ], other . data [ i2 : j2 ])) # type: ignore for _ , i1 , j1 , i2 , j2 in chain . from_iterable ( diff_opcodes ) ] ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : \"\"\"Rich display of all cells in notebook.\"\"\" yield from self . _get_renderables ( expand = True , width = options . max_width // 3 ) def _get_renderables ( self , ** wrap_cols_kwargs : Any ) -> Iterable [ RenderableType ]: \"\"\"Get the Rich renderables, depending on whether `Cells` is a diff or not.\"\"\" if all ( isinstance ( el , tuple ) for el in self . data ): return chain . from_iterable ( Cells . wrap_cols ( val [ 0 ], val [ 1 ], ** wrap_cols_kwargs ) if val [ 0 ] != val [ 1 ] else val [ 0 ] for val in cast ( List [ CellsPair ], self . data ) ) return cast ( List [ Cell ], self . data ) @classmethod def __get_validators__ ( cls ) -> Generator [ Callable [ ... , Any ], None , None ]: \"\"\"Get validators for custom class.\"\"\" yield cls . validate @classmethod def validate ( cls , v : List [ T ]) -> Cells [ T ]: \"\"\"Ensure object is custom defined container.\"\"\" if not isinstance ( v , cls ): return cls ( v ) else : return v @classmethod def wrap_cols ( cls , first_cells : List [ Cell ], last_cells : List [ Cell ], ** cols_kwargs : Any ) -> Sequence [ Columns ]: \"\"\"Wrap the first and second cells into colunmns for iterable.\"\"\" _empty = [ Panel ( Text ( \"<None>\" , justify = \"center\" ), box = box . SIMPLE )] _first = Group ( * first_cells or _empty ) _last = Group ( * last_cells or _empty ) return [ Columns ([ _first , _last ], ** cols_kwargs )] @staticmethod def wrap_git ( first_cells : List [ Cell ], last_cells : List [ Cell ], hash_first : Optional [ str ] = None , hash_last : Optional [ str ] = None , ) -> Sequence [ Cell ]: \"\"\"Wrap git-diff cells in existing notebook.\"\"\" return [ MarkdownCell ( metadata = CellMetadata ( git_hash = hash_first ), source = [ f \"`<<<<<<< { hash_first } `\" ], cell_type = \"markdown\" , ), * first_cells , MarkdownCell ( source = [ \"`=======`\" ], cell_type = \"markdown\" , metadata = CellMetadata (), ), * last_cells , MarkdownCell ( metadata = CellMetadata ( git_hash = hash_last ), source = [ f \"`>>>>>>> { hash_last } `\" ], cell_type = \"markdown\" , ), ] def resolve ( self : Cells [ CellsPair ], * , keep_first_cells : Optional [ bool ] = None , first_id : Optional [ str ] = None , last_id : Optional [ str ] = None , ** kwargs : Any , ) -> List [ Cell ]: \"\"\" Resolve differences between `databooks.data_models.notebook.Cells`. :param keep_first_cells: Whether to keep the cells of the first notebook or not. If `None`, then keep both wrapping the git-diff tags :param first_id: Git hash of first file in conflict :param last_id: Git hash of last file in conflict :param kwargs: (Unused) keyword arguments to keep compatibility with `databooks.data_models.base.resolve` :return: List of cells \"\"\" if keep_first_cells is not None : return list ( chain . from_iterable ( pairs [ not keep_first_cells ] for pairs in self . data ) ) return list ( chain . from_iterable ( Cells . wrap_git ( first_cells = val [ 0 ], last_cells = val [ 1 ], hash_first = first_id , hash_last = last_id , ) if val [ 0 ] != val [ 1 ] else val [ 0 ] for val in self . data ) )","title":"Cells"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.data","text":"Define property data required for collections.UserList class.","title":"data"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.__get_validators__","text":"Get validators for custom class. Source code in databooks/data_models/notebook.py @classmethod def __get_validators__ ( cls ) -> Generator [ Callable [ ... , Any ], None , None ]: \"\"\"Get validators for custom class.\"\"\" yield cls . validate","title":"__get_validators__()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.__init__","text":"Allow passing data as a positional argument when instantiating class. Source code in databooks/data_models/notebook.py def __init__ ( self , elements : Sequence [ T ] = ()) -> None : \"\"\"Allow passing data as a positional argument when instantiating class.\"\"\" super ( Cells , self ) . __init__ ( __root__ = elements )","title":"__init__()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.__iter__","text":"Use list property as iterable. Source code in databooks/data_models/notebook.py def __iter__ ( self ) -> Generator [ Any , None , None ]: \"\"\"Use list property as iterable.\"\"\" return ( el for el in self . data )","title":"__iter__()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.__rich_console__","text":"Rich display of all cells in notebook. Source code in databooks/data_models/notebook.py def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : \"\"\"Rich display of all cells in notebook.\"\"\" yield from self . _get_renderables ( expand = True , width = options . max_width // 3 )","title":"__rich_console__()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.__sub__","text":"Return the difference using difflib.SequenceMatcher . Source code in databooks/data_models/notebook.py def __sub__ ( self : Cells [ Cell ], other : Cells [ Cell ]) -> Cells [ CellsPair ]: \"\"\"Return the difference using `difflib.SequenceMatcher`.\"\"\" if type ( self ) != type ( other ): raise TypeError ( f \"Unsupported operand types for `-`: ` { type ( self ) . __name__ } ` and\" f \" ` { type ( other ) . __name__ } `\" ) # By setting the context to the max number of cells and using # `pathlib.SequenceMatcher.get_grouped_opcodes` we essentially get the same # result as `pathlib.SequenceMatcher.get_opcodes` but in smaller chunks n_context = max ( len ( self ), len ( other )) diff_opcodes = list ( SequenceMatcher ( isjunk = None , a = self , b = other , autojunk = False ) . get_grouped_opcodes ( n_context ) ) if len ( diff_opcodes ) > 1 : raise RuntimeError ( \"Expected one group for opcodes when context size is \" f \" { n_context } for { len ( self ) } and { len ( other ) } cells in\" \" notebooks.\" ) return Cells [ CellsPair ]( [ # https://github.com/python/mypy/issues/9459 tuple (( self . data [ i1 : j1 ], other . data [ i2 : j2 ])) # type: ignore for _ , i1 , j1 , i2 , j2 in chain . from_iterable ( diff_opcodes ) ] )","title":"__sub__()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.resolve","text":"Resolve differences between databooks.data_models.notebook.Cells . Parameters: Name Type Description Default keep_first_cells Optional[bool] Whether to keep the cells of the first notebook or not. If None , then keep both wrapping the git-diff tags None first_id Optional[str] Git hash of first file in conflict None last_id Optional[str] Git hash of last file in conflict None kwargs Any (Unused) keyword arguments to keep compatibility with databooks.data_models.base.resolve {} Returns: Type Description List[Cell] List of cells Source code in databooks/data_models/notebook.py def resolve ( self : Cells [ CellsPair ], * , keep_first_cells : Optional [ bool ] = None , first_id : Optional [ str ] = None , last_id : Optional [ str ] = None , ** kwargs : Any , ) -> List [ Cell ]: \"\"\" Resolve differences between `databooks.data_models.notebook.Cells`. :param keep_first_cells: Whether to keep the cells of the first notebook or not. If `None`, then keep both wrapping the git-diff tags :param first_id: Git hash of first file in conflict :param last_id: Git hash of last file in conflict :param kwargs: (Unused) keyword arguments to keep compatibility with `databooks.data_models.base.resolve` :return: List of cells \"\"\" if keep_first_cells is not None : return list ( chain . from_iterable ( pairs [ not keep_first_cells ] for pairs in self . data ) ) return list ( chain . from_iterable ( Cells . wrap_git ( first_cells = val [ 0 ], last_cells = val [ 1 ], hash_first = first_id , hash_last = last_id , ) if val [ 0 ] != val [ 1 ] else val [ 0 ] for val in self . data ) )","title":"resolve()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.validate","text":"Ensure object is custom defined container. Source code in databooks/data_models/notebook.py @classmethod def validate ( cls , v : List [ T ]) -> Cells [ T ]: \"\"\"Ensure object is custom defined container.\"\"\" if not isinstance ( v , cls ): return cls ( v ) else : return v","title":"validate()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.wrap_cols","text":"Wrap the first and second cells into colunmns for iterable. Source code in databooks/data_models/notebook.py @classmethod def wrap_cols ( cls , first_cells : List [ Cell ], last_cells : List [ Cell ], ** cols_kwargs : Any ) -> Sequence [ Columns ]: \"\"\"Wrap the first and second cells into colunmns for iterable.\"\"\" _empty = [ Panel ( Text ( \"<None>\" , justify = \"center\" ), box = box . SIMPLE )] _first = Group ( * first_cells or _empty ) _last = Group ( * last_cells or _empty ) return [ Columns ([ _first , _last ], ** cols_kwargs )]","title":"wrap_cols()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells.wrap_git","text":"Wrap git-diff cells in existing notebook. Source code in databooks/data_models/notebook.py @staticmethod def wrap_git ( first_cells : List [ Cell ], last_cells : List [ Cell ], hash_first : Optional [ str ] = None , hash_last : Optional [ str ] = None , ) -> Sequence [ Cell ]: \"\"\"Wrap git-diff cells in existing notebook.\"\"\" return [ MarkdownCell ( metadata = CellMetadata ( git_hash = hash_first ), source = [ f \"`<<<<<<< { hash_first } `\" ], cell_type = \"markdown\" , ), * first_cells , MarkdownCell ( source = [ \"`=======`\" ], cell_type = \"markdown\" , metadata = CellMetadata (), ), * last_cells , MarkdownCell ( metadata = CellMetadata ( git_hash = hash_last ), source = [ f \"`>>>>>>> { hash_last } `\" ], cell_type = \"markdown\" , ), ]","title":"wrap_git()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells[Union[CodeCell, RawCell, MarkdownCell]]","text":"","title":"Cells[Union[CodeCell, RawCell, MarkdownCell]]"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells[Union[CodeCell, RawCell, MarkdownCell]].Config","text":"","title":"Config"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells[Union[CodeCell, RawCell, MarkdownCell]].Config.getter_dict","text":"Hack to make object's smell just enough like dicts for validate_model. We can't inherit from Mapping[str, Any] because it upsets cython so we have to implement all methods ourselves.","title":"getter_dict"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells[Union[CodeCell, RawCell, MarkdownCell]].Config.get_field_info","text":"Get properties of FieldInfo from the fields property of the config class.","title":"get_field_info()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells[Union[CodeCell, RawCell, MarkdownCell]].Config.json_dumps","text":"Serialize obj to a JSON formatted str . If skipkeys is true then dict keys that are not basic types ( str , int , float , bool , None ) will be skipped instead of raising a TypeError . If ensure_ascii is false, then the return value can contain non-ASCII characters if they appear in strings contained in obj . Otherwise, all such characters are escaped in JSON strings. If check_circular is false, then the circular reference check for container types will be skipped and a circular reference will result in an OverflowError (or worse). If allow_nan is false, then it will be a ValueError to serialize out of range float values ( nan , inf , -inf ) in strict compliance of the JSON specification, instead of using the JavaScript equivalents ( NaN , Infinity , -Infinity ). If indent is a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. If specified, separators should be an (item_separator, key_separator) tuple. The default is (', ', ': ') if indent is None and (',', ': ') otherwise. To get the most compact JSON representation, you should specify (',', ':') to eliminate whitespace. default(obj) is a function that should return a serializable version of obj or raise TypeError. The default simply raises TypeError. If sort_keys is true (default: False ), then the output of dictionaries will be sorted by key. To use a custom JSONEncoder subclass (e.g. one that overrides the .default() method to serialize additional types), specify it with the cls kwarg; otherwise JSONEncoder is used. Source code in databooks/data_models/notebook.py def dumps ( obj , * , skipkeys = False , ensure_ascii = True , check_circular = True , allow_nan = True , cls = None , indent = None , separators = None , default = None , sort_keys = False , ** kw ): \"\"\"Serialize ``obj`` to a JSON formatted ``str``. If ``skipkeys`` is true then ``dict`` keys that are not basic types (``str``, ``int``, ``float``, ``bool``, ``None``) will be skipped instead of raising a ``TypeError``. If ``ensure_ascii`` is false, then the return value can contain non-ASCII characters if they appear in strings contained in ``obj``. Otherwise, all such characters are escaped in JSON strings. If ``check_circular`` is false, then the circular reference check for container types will be skipped and a circular reference will result in an ``OverflowError`` (or worse). If ``allow_nan`` is false, then it will be a ``ValueError`` to serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) in strict compliance of the JSON specification, instead of using the JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``). If ``indent`` is a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact representation. If specified, ``separators`` should be an ``(item_separator, key_separator)`` tuple. The default is ``(', ', ': ')`` if *indent* is ``None`` and ``(',', ': ')`` otherwise. To get the most compact JSON representation, you should specify ``(',', ':')`` to eliminate whitespace. ``default(obj)`` is a function that should return a serializable version of obj or raise TypeError. The default simply raises TypeError. If *sort_keys* is true (default: ``False``), then the output of dictionaries will be sorted by key. To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the ``.default()`` method to serialize additional types), specify it with the ``cls`` kwarg; otherwise ``JSONEncoder`` is used. \"\"\" # cached encoder if ( not skipkeys and ensure_ascii and check_circular and allow_nan and cls is None and indent is None and separators is None and default is None and not sort_keys and not kw ): return _default_encoder . encode ( obj ) if cls is None : cls = JSONEncoder return cls ( skipkeys = skipkeys , ensure_ascii = ensure_ascii , check_circular = check_circular , allow_nan = allow_nan , indent = indent , separators = separators , default = default , sort_keys = sort_keys , ** kw ) . encode ( obj )","title":"json_dumps()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells[Union[CodeCell, RawCell, MarkdownCell]].Config.json_loads","text":"Deserialize s (a str , bytes or bytearray instance containing a JSON document) to a Python object. object_hook is an optional function that will be called with the result of any object literal decode (a dict ). The return value of object_hook will be used instead of the dict . This feature can be used to implement custom decoders (e.g. JSON-RPC class hinting). object_pairs_hook is an optional function that will be called with the result of any object literal decoded with an ordered list of pairs. The return value of object_pairs_hook will be used instead of the dict . This feature can be used to implement custom decoders. If object_hook is also defined, the object_pairs_hook takes priority. parse_float , if specified, will be called with the string of every JSON float to be decoded. By default this is equivalent to float(num_str). This can be used to use another datatype or parser for JSON floats (e.g. decimal.Decimal). parse_int , if specified, will be called with the string of every JSON int to be decoded. By default this is equivalent to int(num_str). This can be used to use another datatype or parser for JSON integers (e.g. float). parse_constant , if specified, will be called with one of the following strings: -Infinity, Infinity, NaN. This can be used to raise an exception if invalid JSON numbers are encountered. To use a custom JSONDecoder subclass, specify it with the cls kwarg; otherwise JSONDecoder is used. The encoding argument is ignored and deprecated since Python 3.1. Source code in databooks/data_models/notebook.py def loads ( s , * , cls = None , object_hook = None , parse_float = None , parse_int = None , parse_constant = None , object_pairs_hook = None , ** kw ): \"\"\"Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance containing a JSON document) to a Python object. ``object_hook`` is an optional function that will be called with the result of any object literal decode (a ``dict``). The return value of ``object_hook`` will be used instead of the ``dict``. This feature can be used to implement custom decoders (e.g. JSON-RPC class hinting). ``object_pairs_hook`` is an optional function that will be called with the result of any object literal decoded with an ordered list of pairs. The return value of ``object_pairs_hook`` will be used instead of the ``dict``. This feature can be used to implement custom decoders. If ``object_hook`` is also defined, the ``object_pairs_hook`` takes priority. ``parse_float``, if specified, will be called with the string of every JSON float to be decoded. By default this is equivalent to float(num_str). This can be used to use another datatype or parser for JSON floats (e.g. decimal.Decimal). ``parse_int``, if specified, will be called with the string of every JSON int to be decoded. By default this is equivalent to int(num_str). This can be used to use another datatype or parser for JSON integers (e.g. float). ``parse_constant``, if specified, will be called with one of the following strings: -Infinity, Infinity, NaN. This can be used to raise an exception if invalid JSON numbers are encountered. To use a custom ``JSONDecoder`` subclass, specify it with the ``cls`` kwarg; otherwise ``JSONDecoder`` is used. The ``encoding`` argument is ignored and deprecated since Python 3.1. \"\"\" if isinstance ( s , str ): if s . startswith ( ' \\ufeff ' ): raise JSONDecodeError ( \"Unexpected UTF-8 BOM (decode using utf-8-sig)\" , s , 0 ) else : if not isinstance ( s , ( bytes , bytearray )): raise TypeError ( f 'the JSON object must be str, bytes or bytearray, ' f 'not { s . __class__ . __name__ } ' ) s = s . decode ( detect_encoding ( s ), 'surrogatepass' ) if \"encoding\" in kw : import warnings warnings . warn ( \"'encoding' is ignored and deprecated. It will be removed in Python 3.9\" , DeprecationWarning , stacklevel = 2 ) del kw [ 'encoding' ] if ( cls is None and object_hook is None and parse_int is None and parse_float is None and parse_constant is None and object_pairs_hook is None and not kw ): return _default_decoder . decode ( s ) if cls is None : cls = JSONDecoder if object_hook is not None : kw [ 'object_hook' ] = object_hook if object_pairs_hook is not None : kw [ 'object_pairs_hook' ] = object_pairs_hook if parse_float is not None : kw [ 'parse_float' ] = parse_float if parse_int is not None : kw [ 'parse_int' ] = parse_int if parse_constant is not None : kw [ 'parse_constant' ] = parse_constant return cls ( ** kw ) . decode ( s )","title":"json_loads()"},{"location":"data_models/notebook/#databooks.data_models.notebook.Cells[Union[CodeCell, RawCell, MarkdownCell]].Config.prepare_field","text":"Optional hook to check or modify fields during model creation.","title":"prepare_field()"},{"location":"data_models/notebook/#databooks.data_models.notebook.JupyterNotebook","text":"Jupyter notebook. Extra fields yield invalid notebook. Source code in databooks/data_models/notebook.py class JupyterNotebook ( DatabooksBase , extra = Extra . forbid ): \"\"\"Jupyter notebook. Extra fields yield invalid notebook.\"\"\" nbformat : int nbformat_minor : int metadata : NotebookMetadata cells : Cells [ Cell ] def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : \"\"\"Rich display notebook.\"\"\" def _rich ( kernel : str ) -> Text : \"\"\"Display with `kernel` theme, horizontal padding and right-justified.\"\"\" return Text ( kernel , style = \"kernel\" , justify = \"right\" ) kernelspec = self . metadata . dict () . get ( \"kernelspec\" , {}) if isinstance ( kernelspec , tuple ): # check if this is a `DiffCells` lang_first , lang_last = ( ks . get ( \"language\" , \"text\" ) for ks in kernelspec ) nb_lang = lang_first if lang_first == lang_last else \"text\" if any ( \"display_name\" in ks . keys () for ks in kernelspec ): kernel_first , kernel_last = [ _rich ( ks [ \"display_name\" ]) for ks in kernelspec ] yield Columns ( [ kernel_first , kernel_last ], expand = True , width = options . max_width // 3 , ) if kernel_first != kernel_last else kernel_first else : nb_lang = kernelspec . get ( \"language\" , \"text\" ) if \"display_name\" in kernelspec . keys (): yield _rich ( kernelspec [ \"display_name\" ]) for cell in self . cells : if isinstance ( cell , CodeCell ): cell . metadata = CellMetadata ( ** cell . metadata . dict (), lang = nb_lang ) yield self . cells @classmethod def parse_file ( cls , path : Path | str , ** parse_kwargs : Any ) -> JupyterNotebook : \"\"\"Parse notebook from a path.\"\"\" content_arg = parse_kwargs . pop ( \"content_type\" , None ) if content_arg is not None : raise ValueError ( f \"Value of `content_type` must be `json` (default), got ` { content_arg } `\" ) return super ( JupyterNotebook , cls ) . parse_file ( path = path , content_type = \"json\" , ** parse_kwargs ) def write ( self , path : Path | str , overwrite : bool = False , ** json_kwargs : Any ) -> None : \"\"\"Write notebook to disk.\"\"\" path = Path ( path ) if not isinstance ( path , Path ) else path json_kwargs = { \"indent\" : 2 , ** json_kwargs } if path . is_file () and not overwrite : raise ValueError ( f \"File exists at { path } exists. Specify `overwrite = True`.\" ) _ , _ , validation_error = validate_model ( self . __class__ , self . dict ()) if validation_error : raise validation_error with path . open ( \"w\" ) as f : json . dump ( self . dict (), fp = f , ** json_kwargs ) def clear_metadata ( self , * , notebook_metadata_keep : Sequence [ str ] = None , notebook_metadata_remove : Sequence [ str ] = None , ** cell_kwargs : Any , ) -> None : \"\"\" Clear notebook and cell metadata. :param notebook_metadata_keep: Metadata values to keep - simply pass an empty sequence (i.e.: `()`) to remove all extra fields. :param notebook_metadata_remove: Metadata values to remove :param cell_kwargs: keyword arguments to be passed to each cell's `databooks.data_models.cell.BaseCell.clear_metadata` :return: \"\"\" nargs = sum ( ( notebook_metadata_keep is not None , notebook_metadata_remove is not None ) ) if nargs != 1 : raise ValueError ( \"Exactly one of `notebook_metadata_keep` or `notebook_metadata_remove`\" f \" must be passed, got { nargs } arguments.\" ) if notebook_metadata_keep is not None : notebook_metadata_remove = tuple ( field for field , _ in self . metadata if field not in notebook_metadata_keep ) self . metadata . remove_fields ( notebook_metadata_remove ) # type: ignore if len ( cell_kwargs ) > 0 : _clean_cells = deepcopy ( self . cells ) for cell in _clean_cells : cell . clear_fields ( ** cell_kwargs ) self . cells = _clean_cells","title":"JupyterNotebook"},{"location":"data_models/notebook/#databooks.data_models.notebook.JupyterNotebook.__rich_console__","text":"Rich display notebook. Source code in databooks/data_models/notebook.py def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : \"\"\"Rich display notebook.\"\"\" def _rich ( kernel : str ) -> Text : \"\"\"Display with `kernel` theme, horizontal padding and right-justified.\"\"\" return Text ( kernel , style = \"kernel\" , justify = \"right\" ) kernelspec = self . metadata . dict () . get ( \"kernelspec\" , {}) if isinstance ( kernelspec , tuple ): # check if this is a `DiffCells` lang_first , lang_last = ( ks . get ( \"language\" , \"text\" ) for ks in kernelspec ) nb_lang = lang_first if lang_first == lang_last else \"text\" if any ( \"display_name\" in ks . keys () for ks in kernelspec ): kernel_first , kernel_last = [ _rich ( ks [ \"display_name\" ]) for ks in kernelspec ] yield Columns ( [ kernel_first , kernel_last ], expand = True , width = options . max_width // 3 , ) if kernel_first != kernel_last else kernel_first else : nb_lang = kernelspec . get ( \"language\" , \"text\" ) if \"display_name\" in kernelspec . keys (): yield _rich ( kernelspec [ \"display_name\" ]) for cell in self . cells : if isinstance ( cell , CodeCell ): cell . metadata = CellMetadata ( ** cell . metadata . dict (), lang = nb_lang ) yield self . cells","title":"__rich_console__()"},{"location":"data_models/notebook/#databooks.data_models.notebook.JupyterNotebook.clear_metadata","text":"Clear notebook and cell metadata. Parameters: Name Type Description Default notebook_metadata_keep Sequence[str] Metadata values to keep - simply pass an empty sequence (i.e.: () ) to remove all extra fields. None notebook_metadata_remove Sequence[str] Metadata values to remove None cell_kwargs Any keyword arguments to be passed to each cell's databooks.data_models.cell.BaseCell.clear_metadata {} Returns: Type Description None Source code in databooks/data_models/notebook.py def clear_metadata ( self , * , notebook_metadata_keep : Sequence [ str ] = None , notebook_metadata_remove : Sequence [ str ] = None , ** cell_kwargs : Any , ) -> None : \"\"\" Clear notebook and cell metadata. :param notebook_metadata_keep: Metadata values to keep - simply pass an empty sequence (i.e.: `()`) to remove all extra fields. :param notebook_metadata_remove: Metadata values to remove :param cell_kwargs: keyword arguments to be passed to each cell's `databooks.data_models.cell.BaseCell.clear_metadata` :return: \"\"\" nargs = sum ( ( notebook_metadata_keep is not None , notebook_metadata_remove is not None ) ) if nargs != 1 : raise ValueError ( \"Exactly one of `notebook_metadata_keep` or `notebook_metadata_remove`\" f \" must be passed, got { nargs } arguments.\" ) if notebook_metadata_keep is not None : notebook_metadata_remove = tuple ( field for field , _ in self . metadata if field not in notebook_metadata_keep ) self . metadata . remove_fields ( notebook_metadata_remove ) # type: ignore if len ( cell_kwargs ) > 0 : _clean_cells = deepcopy ( self . cells ) for cell in _clean_cells : cell . clear_fields ( ** cell_kwargs ) self . cells = _clean_cells","title":"clear_metadata()"},{"location":"data_models/notebook/#databooks.data_models.notebook.JupyterNotebook.parse_file","text":"Parse notebook from a path. Source code in databooks/data_models/notebook.py @classmethod def parse_file ( cls , path : Path | str , ** parse_kwargs : Any ) -> JupyterNotebook : \"\"\"Parse notebook from a path.\"\"\" content_arg = parse_kwargs . pop ( \"content_type\" , None ) if content_arg is not None : raise ValueError ( f \"Value of `content_type` must be `json` (default), got ` { content_arg } `\" ) return super ( JupyterNotebook , cls ) . parse_file ( path = path , content_type = \"json\" , ** parse_kwargs )","title":"parse_file()"},{"location":"data_models/notebook/#databooks.data_models.notebook.JupyterNotebook.write","text":"Write notebook to disk. Source code in databooks/data_models/notebook.py def write ( self , path : Path | str , overwrite : bool = False , ** json_kwargs : Any ) -> None : \"\"\"Write notebook to disk.\"\"\" path = Path ( path ) if not isinstance ( path , Path ) else path json_kwargs = { \"indent\" : 2 , ** json_kwargs } if path . is_file () and not overwrite : raise ValueError ( f \"File exists at { path } exists. Specify `overwrite = True`.\" ) _ , _ , validation_error = validate_model ( self . __class__ , self . dict ()) if validation_error : raise validation_error with path . open ( \"w\" ) as f : json . dump ( self . dict (), fp = f , ** json_kwargs )","title":"write()"},{"location":"data_models/notebook/#databooks.data_models.notebook.NotebookMetadata","text":"Notebook metadata. Empty by default but can accept extra fields. Source code in databooks/data_models/notebook.py class NotebookMetadata ( DatabooksBase ): \"\"\"Notebook metadata. Empty by default but can accept extra fields.\"\"\"","title":"NotebookMetadata"},{"location":"usage/cicd/","text":"CI/CD CI/CD essentially runs some code in a remote server, triggered by git events. In these runs, one could trigger other events or simply check whether the code is up to standards. This is a nice way to make sure that all the code that is in your git repo passes all quality checks. GitHub Actions GitHub Actions are a GitHub-hosted solution for CI/CD. All you need to get started is a file in project_root/.github/workflows/nb-meta.yml . An example workflow to clean any notebook metadata and commit changes at every push: name: 'nb-meta' on: [push] jobs: nb-meta: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v2 with: python-version: 3.8 - name: Configure git user run: | git config --global user.name 'github-actions[bot]' git config --global user.email 'github-actions[bot]@users.noreply.github.com' - name: Install dependencies and clean metadata run: | pip install databooks databooks meta . --overwrite - name: Commit changes and push run: | git commit -am \"Automated commit - clean notebook metadata\" git push Alternatively, one can choose to avoid having CI systems making code changes. In that case, we can only check whether notebooks have any unwanted metadata. While running checks, one can also use databooks assert to check for any desired metadata. name: 'nb-checks' on: [push] jobs: nb-meta: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v2 with: python-version: 3.8 - name: Install dependencies and check metadata run: | pip install databooks databooks meta . --check databooks assert . --recipe seq-exec","title":"CI/CD"},{"location":"usage/cicd/#cicd","text":"CI/CD essentially runs some code in a remote server, triggered by git events. In these runs, one could trigger other events or simply check whether the code is up to standards. This is a nice way to make sure that all the code that is in your git repo passes all quality checks.","title":"CI/CD"},{"location":"usage/cicd/#github-actions","text":"GitHub Actions are a GitHub-hosted solution for CI/CD. All you need to get started is a file in project_root/.github/workflows/nb-meta.yml . An example workflow to clean any notebook metadata and commit changes at every push: name: 'nb-meta' on: [push] jobs: nb-meta: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v2 with: python-version: 3.8 - name: Configure git user run: | git config --global user.name 'github-actions[bot]' git config --global user.email 'github-actions[bot]@users.noreply.github.com' - name: Install dependencies and clean metadata run: | pip install databooks databooks meta . --overwrite - name: Commit changes and push run: | git commit -am \"Automated commit - clean notebook metadata\" git push Alternatively, one can choose to avoid having CI systems making code changes. In that case, we can only check whether notebooks have any unwanted metadata. While running checks, one can also use databooks assert to check for any desired metadata. name: 'nb-checks' on: [push] jobs: nb-meta: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v2 with: python-version: 3.8 - name: Install dependencies and check metadata run: | pip install databooks databooks meta . --check databooks assert . --recipe seq-exec","title":"GitHub Actions"},{"location":"usage/cli/","text":"CLI tool The most straightforward way to start using databooks is by using the terminal. It's the default way of running the commands and how you've probably seen databooks being used before. However, using the terminal can be error-prone and result in \"dirty\" notebooks in your git repo. Check CLI documentation for more information. A safer alternative is to automate your databooks commands, by setting up CI in your repo or pre-commit hooks.","title":"CLI"},{"location":"usage/cli/#cli-tool","text":"The most straightforward way to start using databooks is by using the terminal. It's the default way of running the commands and how you've probably seen databooks being used before. However, using the terminal can be error-prone and result in \"dirty\" notebooks in your git repo. Check CLI documentation for more information. A safer alternative is to automate your databooks commands, by setting up CI in your repo or pre-commit hooks.","title":"CLI tool"},{"location":"usage/configuration/","text":"Configuration Instead of passing the same parameters every time when running a command, it is also possible to set up a configuration that will be read and override the defaults. The order of priority (from higher priority to lower) User input arguments in the CLI Configuration file Defaults So it's still possible to override the configuration file via CLI parameters (as expected). What can I configure? All CLI parameters are actually configurable, so you can pass specify anything that is also available to you via the UI, with one exception: the required PATHS argument. This is because the PATHS argument is also used for finding your configuration (see how can I use it for more information). Info Remember that flags are parsed as boolean values. So you can specify --verbose on the configuration as verbose=true . How does it look like? The configuration file is a pyproject.toml file that you can place at the root of your project. There, you can specify values for either command under the [tool.databooks.<command>] . So if, for example, the desired behavior is databooks meta Remove outputs Don't remove execution count Always overwrite files databooks fix Keep notebook metadata from base (not head ) databooks assert Always check that notebook has less than 10 cells The pyproject.toml file would look like [tool.databooks.meta] rm-outs = true rm_exec = false overwrite = true [tool.databooks.fix] metadata-head = false [tool.databooks.assert] expr = [ \"len(nb.cells) < 10\" ] How can I use it? There are 2 ways to specify the configuration file: explicitly and implicitly. You can explicitly specify the pyproject.toml via the --config parameter. If none is specified, then databooks will look for a pyproject.toml in your project. databooks will look for the configuration file by first finding the common directory between all the target paths and from there recursively go to the parent directories until either finding the configuration file or the root of the git repo. That way, you can have multiple configuration files and depending on where your notebooks are located the correct values will be used (think monorepo ). Tip databooks has a verbose concept that will print more information to the terminal if desired. For debugging purposes one can still increase the verbosity by setting and environment variable LOG_LEVEL to DEBUG . That way, one can get information, among many other things, of the configuration file used.","title":"Configuration"},{"location":"usage/configuration/#configuration","text":"Instead of passing the same parameters every time when running a command, it is also possible to set up a configuration that will be read and override the defaults. The order of priority (from higher priority to lower) User input arguments in the CLI Configuration file Defaults So it's still possible to override the configuration file via CLI parameters (as expected).","title":"Configuration"},{"location":"usage/configuration/#what-can-i-configure","text":"All CLI parameters are actually configurable, so you can pass specify anything that is also available to you via the UI, with one exception: the required PATHS argument. This is because the PATHS argument is also used for finding your configuration (see how can I use it for more information). Info Remember that flags are parsed as boolean values. So you can specify --verbose on the configuration as verbose=true .","title":"What can I configure?"},{"location":"usage/configuration/#how-does-it-look-like","text":"The configuration file is a pyproject.toml file that you can place at the root of your project. There, you can specify values for either command under the [tool.databooks.<command>] . So if, for example, the desired behavior is databooks meta Remove outputs Don't remove execution count Always overwrite files databooks fix Keep notebook metadata from base (not head ) databooks assert Always check that notebook has less than 10 cells The pyproject.toml file would look like [tool.databooks.meta] rm-outs = true rm_exec = false overwrite = true [tool.databooks.fix] metadata-head = false [tool.databooks.assert] expr = [ \"len(nb.cells) < 10\" ]","title":"How does it look like?"},{"location":"usage/configuration/#how-can-i-use-it","text":"There are 2 ways to specify the configuration file: explicitly and implicitly. You can explicitly specify the pyproject.toml via the --config parameter. If none is specified, then databooks will look for a pyproject.toml in your project. databooks will look for the configuration file by first finding the common directory between all the target paths and from there recursively go to the parent directories until either finding the configuration file or the root of the git repo. That way, you can have multiple configuration files and depending on where your notebooks are located the correct values will be used (think monorepo ). Tip databooks has a verbose concept that will print more information to the terminal if desired. For debugging purposes one can still increase the verbosity by setting and environment variable LOG_LEVEL to DEBUG . That way, one can get information, among many other things, of the configuration file used.","title":"How can I use it?"},{"location":"usage/overview/","text":"Usage databooks is a tool designed to make the life of Jupyter notebook users easier, especially when it comes to sharing and versioning notebooks. That is because Jupyter notebooks are actually JSON files, with extra metadata that are useful for Jupyter but unnecessary for many users. When committing notebooks you commit all the metadata that may cause some issues down the line. This is where databooks comes in. The package currently has 3 main features, exposed as CLI commands databooks meta : to remove unnecessary notebook metadata that can cause git conflicts databooks fix : to fix conflicts after they've occurred, by parsing versions of the conflicting file and computing its difference in a Jupyter-friendly way, so you (user) can manually resolve them in the Jupyter terminal databooks assert : to assert that the notebook metadata actually conforms to desired values - ensure that notebook has sequential execution count, tags, etc. databooks meta The only thing you need to pass is a path. We have sensible defaults to do the rest. databooks meta path/to/notebooks With that, for each notebook in the path, by default: It will remove execution counts It won't remove cell outputs It will remove metadata from all cells (such as cell tags or ids) It will remove all metadata from your notebook (including kernel information) It won't overwrite files for you Nonetheless, the tool is highly configurable. You could choose to remove cell outputs by passing --rm-outs . Or if there is some metadata you'd like to keep, such as cell tags, you can do so by passing --cell-meta-keep tags . Also, if you do want to save the clean notebook you can either pass a prefix ( --prefix ... ) or a suffix ( --suffix ... ) that will be added before writing the file, or you can simply overwrite the source file ( --overwrite ). databooks fix In databooks meta we try to avoid git conflicts. In databooks fix we fix conflicts after they have occurred. Similar to databooks meta ... , the only required argument here is a path. databooks fix path/to/notebooks For each notebook in the path that has git conflicts : It will keep the metadata from the notebook in HEAD For the conflicting cells, it will wrap some special cells around the differences, like in normal git conflicts Similarly to databooks meta , the default behavior can be changed by passing a configuration pyproject.toml file or specifying the CLI arguments. You could, for instance, keep the metadata from the notebook in BASE (as opposed to HEAD ). If you know you only care about the notebook cells in HEAD or BASE , then you could pass --cells-head or --no-cells-head and not worry about fixing conflicted cells in Jupyter. You can also pass a special --cell-fields-ignore parameter , that will remove the cell metadata from both versions of the conflicting notebook before comparing them. This is because depending on your Jupyter version you may have an id field, that will be unique for each cell. That is, all the cells will be considered different even if they have the same source and outputs as their id s are different. By removing id and execution_count (we'll do this by default) we only compare the actual code and outputs to determine if the cells have changed or not. Note If a notebook with conflicts (thus not valid JSON/Jupyter) is committed to the repo, databooks fix will not consider the file as something to fix - same behavior as git . Fun fact \"Fix\" may be a misnomer: the \"broken\" JSON in the notebook is not actually fixed - instead we compare the versions of the notebook that caused the conflict. databooks assert In databooks meta , we remove unwanted metadata. But sometimes we may still want some metadata (such as cell tags), or more than that, we may want the metadata to have certain values. This is where databooks assert comes in. We can use this command to ensure that the metadata is present and has the desired values. databooks assert is akin to (and inspired by) Python's assert . Therefore, the user must pass a path and a string with the expression to be evaluated for each notebook. databooks assert path/to/notebooks --expr \"python expression to assert on notebooks\" This can be used, for example, to make sure that the notebook cells were executed in order. Or that we have markdown cells, or to set a maximum number of cells for each notebook. Evidently, there are some limitations to the expressions that a user can pass. Variables in scope: All the variables available for in your assert expressions are subclasses of Pydantic models . Therefore, you can use these models as regular python objects (i.e.: to access the cell types, one could write [cell.cell_type for cell in nb.cells] ). For convenience's sake, follows a list of currently supported variables that can be used in assert expressions: nb : Jupyter notebook found in path raw_cells : notebook \"raw\" cells md_cells : notebook markdown cells code_cells : notebook code cells exec_cells : executed notebook code cells Built-in functions: all any enumerate filter getattr hasattr len list range sorted These limitations are designed to allow anyone to use databooks assert safely. This is because we use built-in's eval , and eval is really dangerous . To mitigate that (and for your safety), we actually parse the string and only allow a couple of operations to happen. Check out our tests to see what is and isn't allowed and see the source to see how that happens! It's also relevant to mention that to avoid repetitive typing you can configure the tool to omit the source string. An even more powerful method is to combine it with pre-commit or CI/CD. Check out the rest of the \"Usage\" section for more info! Recipes It can be a bit repetitive and tedious to write out expressions to be asserted. Or even hard to think of how to express these assertions about notebooks. With that in mind, we also include \"user recipes\". These recipes store some useful expressions to be checked, to be used both as shorthand of other expressions and inspiration for you to come up with your own recipe! Feel free to submit a PR with your recipe or open an issue if you're having issues coming up with a recipe for your goal. has-tags Description: Assert that there is at least one cell with tags. Source: any(getattr(cell.metadata, 'tags', []) for cell in nb.cells) has-tags-code Description: Assert that there is at least one code cell with tags. Source: any(getattr(cell.metadata, 'tags', []) for cell in code_cells) max-cells Description: Assert that there are less than 64 cells in the notebook. Source: len(nb.cells) < 64 no-empty-code Description: Assert that there are no empty code cells in the notebook. Source: all(cell.source for cell in code_cells) seq-exec Description: Assert that the executed code cells were executed sequentially (similar effect to when you 'restart kernel and run all cells'). Source: [c.execution_count for c in exec_cells] == list(range(1, len(exec_cells) + 1)) seq-increase Description: Assert that the executed code cells were executed in increasing order. Source: [c.execution_count for c in exec_cells] == sorted([c.execution_count for c in exec_cells]) startswith-md Description: Assert that the first cell in notebook is a markdown cell. Source: nb.cells[0].cell_type == 'markdown' Tip If your use case is more complex and cannot be translated into a single expression, you can always download databooks and use it as a part of your script!","title":"Overview"},{"location":"usage/overview/#usage","text":"databooks is a tool designed to make the life of Jupyter notebook users easier, especially when it comes to sharing and versioning notebooks. That is because Jupyter notebooks are actually JSON files, with extra metadata that are useful for Jupyter but unnecessary for many users. When committing notebooks you commit all the metadata that may cause some issues down the line. This is where databooks comes in. The package currently has 3 main features, exposed as CLI commands databooks meta : to remove unnecessary notebook metadata that can cause git conflicts databooks fix : to fix conflicts after they've occurred, by parsing versions of the conflicting file and computing its difference in a Jupyter-friendly way, so you (user) can manually resolve them in the Jupyter terminal databooks assert : to assert that the notebook metadata actually conforms to desired values - ensure that notebook has sequential execution count, tags, etc.","title":"Usage"},{"location":"usage/overview/#databooks-meta","text":"The only thing you need to pass is a path. We have sensible defaults to do the rest. databooks meta path/to/notebooks With that, for each notebook in the path, by default: It will remove execution counts It won't remove cell outputs It will remove metadata from all cells (such as cell tags or ids) It will remove all metadata from your notebook (including kernel information) It won't overwrite files for you Nonetheless, the tool is highly configurable. You could choose to remove cell outputs by passing --rm-outs . Or if there is some metadata you'd like to keep, such as cell tags, you can do so by passing --cell-meta-keep tags . Also, if you do want to save the clean notebook you can either pass a prefix ( --prefix ... ) or a suffix ( --suffix ... ) that will be added before writing the file, or you can simply overwrite the source file ( --overwrite ).","title":"databooks meta"},{"location":"usage/overview/#databooks-fix","text":"In databooks meta we try to avoid git conflicts. In databooks fix we fix conflicts after they have occurred. Similar to databooks meta ... , the only required argument here is a path. databooks fix path/to/notebooks For each notebook in the path that has git conflicts : It will keep the metadata from the notebook in HEAD For the conflicting cells, it will wrap some special cells around the differences, like in normal git conflicts Similarly to databooks meta , the default behavior can be changed by passing a configuration pyproject.toml file or specifying the CLI arguments. You could, for instance, keep the metadata from the notebook in BASE (as opposed to HEAD ). If you know you only care about the notebook cells in HEAD or BASE , then you could pass --cells-head or --no-cells-head and not worry about fixing conflicted cells in Jupyter. You can also pass a special --cell-fields-ignore parameter , that will remove the cell metadata from both versions of the conflicting notebook before comparing them. This is because depending on your Jupyter version you may have an id field, that will be unique for each cell. That is, all the cells will be considered different even if they have the same source and outputs as their id s are different. By removing id and execution_count (we'll do this by default) we only compare the actual code and outputs to determine if the cells have changed or not. Note If a notebook with conflicts (thus not valid JSON/Jupyter) is committed to the repo, databooks fix will not consider the file as something to fix - same behavior as git . Fun fact \"Fix\" may be a misnomer: the \"broken\" JSON in the notebook is not actually fixed - instead we compare the versions of the notebook that caused the conflict.","title":"databooks fix"},{"location":"usage/overview/#databooks-assert","text":"In databooks meta , we remove unwanted metadata. But sometimes we may still want some metadata (such as cell tags), or more than that, we may want the metadata to have certain values. This is where databooks assert comes in. We can use this command to ensure that the metadata is present and has the desired values. databooks assert is akin to (and inspired by) Python's assert . Therefore, the user must pass a path and a string with the expression to be evaluated for each notebook. databooks assert path/to/notebooks --expr \"python expression to assert on notebooks\" This can be used, for example, to make sure that the notebook cells were executed in order. Or that we have markdown cells, or to set a maximum number of cells for each notebook. Evidently, there are some limitations to the expressions that a user can pass. Variables in scope: All the variables available for in your assert expressions are subclasses of Pydantic models . Therefore, you can use these models as regular python objects (i.e.: to access the cell types, one could write [cell.cell_type for cell in nb.cells] ). For convenience's sake, follows a list of currently supported variables that can be used in assert expressions: nb : Jupyter notebook found in path raw_cells : notebook \"raw\" cells md_cells : notebook markdown cells code_cells : notebook code cells exec_cells : executed notebook code cells Built-in functions: all any enumerate filter getattr hasattr len list range sorted These limitations are designed to allow anyone to use databooks assert safely. This is because we use built-in's eval , and eval is really dangerous . To mitigate that (and for your safety), we actually parse the string and only allow a couple of operations to happen. Check out our tests to see what is and isn't allowed and see the source to see how that happens! It's also relevant to mention that to avoid repetitive typing you can configure the tool to omit the source string. An even more powerful method is to combine it with pre-commit or CI/CD. Check out the rest of the \"Usage\" section for more info!","title":"databooks assert"},{"location":"usage/overview/#recipes","text":"It can be a bit repetitive and tedious to write out expressions to be asserted. Or even hard to think of how to express these assertions about notebooks. With that in mind, we also include \"user recipes\". These recipes store some useful expressions to be checked, to be used both as shorthand of other expressions and inspiration for you to come up with your own recipe! Feel free to submit a PR with your recipe or open an issue if you're having issues coming up with a recipe for your goal.","title":"Recipes"},{"location":"usage/overview/#has-tags","text":"Description: Assert that there is at least one cell with tags. Source: any(getattr(cell.metadata, 'tags', []) for cell in nb.cells)","title":"has-tags"},{"location":"usage/overview/#has-tags-code","text":"Description: Assert that there is at least one code cell with tags. Source: any(getattr(cell.metadata, 'tags', []) for cell in code_cells)","title":"has-tags-code"},{"location":"usage/overview/#max-cells","text":"Description: Assert that there are less than 64 cells in the notebook. Source: len(nb.cells) < 64","title":"max-cells"},{"location":"usage/overview/#no-empty-code","text":"Description: Assert that there are no empty code cells in the notebook. Source: all(cell.source for cell in code_cells)","title":"no-empty-code"},{"location":"usage/overview/#seq-exec","text":"Description: Assert that the executed code cells were executed sequentially (similar effect to when you 'restart kernel and run all cells'). Source: [c.execution_count for c in exec_cells] == list(range(1, len(exec_cells) + 1))","title":"seq-exec"},{"location":"usage/overview/#seq-increase","text":"Description: Assert that the executed code cells were executed in increasing order. Source: [c.execution_count for c in exec_cells] == sorted([c.execution_count for c in exec_cells])","title":"seq-increase"},{"location":"usage/overview/#startswith-md","text":"Description: Assert that the first cell in notebook is a markdown cell. Source: nb.cells[0].cell_type == 'markdown' Tip If your use case is more complex and cannot be translated into a single expression, you can always download databooks and use it as a part of your script!","title":"startswith-md"},{"location":"usage/precommit/","text":"Pre-commit hooks Another alternative is to try to catch code quality issues before any code is even sent to the remote git repo. Pre-commit hooks are essentially actions that are taken right before code is committed to your (local) repo. Pre-commit package There are different ways to create new hooks to your git repo. pre-commit is a package to easily config pre-commit hooks, and store them in a very readable manner. Installation To install, simply run: pip install pre-commit Usage Configuration databooks meta To use pre-commit with databooks meta , create a .pre-commit-config.yaml in the root of your project. There, include repos : - repo : https://github.com/datarootsio/databooks rev : 1.0.1 hooks : - id : databooks-meta databooks repo has minimal configuration (such as the meta command). The rev parameter indicates the version to use and args indicate additional arguments to pass to the tool. The pre-commit tool doesn't actually commit any changes if the staged files are modified. Therefore, if there is any unwanted metadata at the time of committing the changes, the files would be modified, no commit would be made, and it'd be up to the developer to inspect the changes, add them and commit. That's why we specify args: [\"--overwrite\"] by default. databooks assert To use databooks assert you must pass similar values, added with an args field to write your checks. Those can be a recipe or an expression , similarly to what would be done via the CLI. The difference here is that we pass an \"extra check\" that evaluates to True (namely databooks assert --expr \"True\" ) to allow a user to commit in case there are no tests in the configuration file. repos : - repo : https://github.com/datarootsio/databooks rev : 1.0.1 hooks : - id : databooks-assert args : [ '--expr' , 'len(nb.cells) < 10' , '--recipe' , 'seq-exec' ] Running Once the configuration is in place all the user needs to do to trigger pre-commit is to commit changes normally $ git add path/to/notebook.ipynb $ git commit -m 'a clear message' databooks-assert.........................................................Failed - hook id: databooks-assert - exit code: 1 [ 12 :24:10 ] INFO path/to/notebook.ipynb failed 0 of 3 checks. affirm.py:214 Running assert checks \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100 % 0 :00:00 INFO Found issues in notebook metadata for 1 out of 1 cli.py:257 notebooks. databooks-meta...........................................................Failed - hook id: databooks-meta - files were modified by this hook [ 23 :24:11 ] WARNING 1 files will be overwritten cli.py:149 Removing metadata \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100 % 0 :00:00 INFO The metadata of 1 out of 1 notebooks were cli.py:185 removed! Alternatively, one could run pre-commit run to manually run the same command that is triggered right before committing changes. Or, one could run pre-commit run --all-files to run the pre-commit hooks in all files (regardless if the files have been staged or not). The latter is useful as a first-run to ensure consistency across the git repo or in CI.","title":"Pre-commit hooks"},{"location":"usage/precommit/#pre-commit-hooks","text":"Another alternative is to try to catch code quality issues before any code is even sent to the remote git repo. Pre-commit hooks are essentially actions that are taken right before code is committed to your (local) repo.","title":"Pre-commit hooks"},{"location":"usage/precommit/#pre-commit-package","text":"There are different ways to create new hooks to your git repo. pre-commit is a package to easily config pre-commit hooks, and store them in a very readable manner.","title":"Pre-commit package"},{"location":"usage/precommit/#installation","text":"To install, simply run: pip install pre-commit","title":"Installation"},{"location":"usage/precommit/#usage","text":"","title":"Usage"},{"location":"usage/precommit/#configuration","text":"","title":"Configuration"},{"location":"usage/precommit/#databooks-meta","text":"To use pre-commit with databooks meta , create a .pre-commit-config.yaml in the root of your project. There, include repos : - repo : https://github.com/datarootsio/databooks rev : 1.0.1 hooks : - id : databooks-meta databooks repo has minimal configuration (such as the meta command). The rev parameter indicates the version to use and args indicate additional arguments to pass to the tool. The pre-commit tool doesn't actually commit any changes if the staged files are modified. Therefore, if there is any unwanted metadata at the time of committing the changes, the files would be modified, no commit would be made, and it'd be up to the developer to inspect the changes, add them and commit. That's why we specify args: [\"--overwrite\"] by default.","title":"databooks meta"},{"location":"usage/precommit/#databooks-assert","text":"To use databooks assert you must pass similar values, added with an args field to write your checks. Those can be a recipe or an expression , similarly to what would be done via the CLI. The difference here is that we pass an \"extra check\" that evaluates to True (namely databooks assert --expr \"True\" ) to allow a user to commit in case there are no tests in the configuration file. repos : - repo : https://github.com/datarootsio/databooks rev : 1.0.1 hooks : - id : databooks-assert args : [ '--expr' , 'len(nb.cells) < 10' , '--recipe' , 'seq-exec' ]","title":"databooks assert"},{"location":"usage/precommit/#running","text":"Once the configuration is in place all the user needs to do to trigger pre-commit is to commit changes normally $ git add path/to/notebook.ipynb $ git commit -m 'a clear message' databooks-assert.........................................................Failed - hook id: databooks-assert - exit code: 1 [ 12 :24:10 ] INFO path/to/notebook.ipynb failed 0 of 3 checks. affirm.py:214 Running assert checks \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100 % 0 :00:00 INFO Found issues in notebook metadata for 1 out of 1 cli.py:257 notebooks. databooks-meta...........................................................Failed - hook id: databooks-meta - files were modified by this hook [ 23 :24:11 ] WARNING 1 files will be overwritten cli.py:149 Removing metadata \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100 % 0 :00:00 INFO The metadata of 1 out of 1 notebooks were cli.py:185 removed! Alternatively, one could run pre-commit run to manually run the same command that is triggered right before committing changes. Or, one could run pre-commit run --all-files to run the pre-commit hooks in all files (regardless if the files have been staged or not). The latter is useful as a first-run to ensure consistency across the git repo or in CI.","title":"Running"},{"location":"coverage/","text":".md-content { max-width: none !important; } article h1, article > a { display: none; } var coviframe = document.getElementById(\"coviframe\"); function resizeIframe() { coviframe.style.height = coviframe.contentWindow.document.documentElement.offsetHeight + 'px'; } coviframe.contentWindow.document.body.onclick = function() { coviframe.contentWindow.location.reload(); }","title":"Coverage report"}]}